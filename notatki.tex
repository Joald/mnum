\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{polski}
\usepackage{textcomp}
\usepackage{color}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{amsthm}
\usepackage{amsmath}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
	language=Bash,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	escapeinside={(*}{*)},          % if you want to add LaTeX within your code
	tabsize=4
}

\title{Notatki z Metod numerycznych }
\author{Jacek Olczyk}
\date{October 2018}

\begin{document}
	\maketitle
	\part{Wykład}
	\section{Rozwiązywanie układów równań liniowych}
	\paragraph{Znane metody}
	\begin{itemize}
		\item $Ax=b, A\in\mathbb R^{n\times n}$
		\item Algorytm rokładem LU (elim. Gaussa) z wybraniem el. gł $O(\frac{2}{3}N^3)$.
		\item 1. złota myśl numeryka: Co zrobić jeśli zadanie jest za trudne? Zmienić zadanie
		\item Zamiast rozwiązywać układ równań, przybliżamy go
		\item Czy da się szybciej niż Gauss, który jest $O(n^3)$? To jak nisko da się zejść to problem otwarty, ale istnieją algorytmy lepsze niż sześcian.
	\end{itemize}

\section{Przybliżone rozwiązywanie układów równań}
\begin{itemize}
	\item Niech $ A=M-Z $, wtedy $ Ax=Mx-Zx=b $, zatem $ Mx=Zx+b $
	\item TODO Metoda iteracji prostej Banacha $Mx_{n+1} = Zx_{n}+b$
	\item Jeśli wybierzemy $ M $ tak, by układ z macierzą $ M $ można było tanio rozwiązać, wtedy iteracja też będzie tania
	\item Chcemy, żeby $ M $ było dobrym przybliżeniem $ A $, ale nie aż tak łatwo że
	\item Metoda Jacobiego $$a_{kk}x_{k}^{n+1}=b - \sum_{j\not=k} a_{kj}x_{j}^n$$
	\item inny pomysł, metoda Gaussa-Seidela: $a_{kk}x_k^{(n+1)} = b_k - \sum_{j<k}a_{kj}x_{j}^{(n+1)} - \sum_{j >k}a_{kj}x_{j}^{(n)}$
	\item Uwaga: fakt życiowy. Gdy $n$ jest bardzo duże, wówczas w $A$ jest zazwyczaj bardzo dużo zer, o ile układ pochodzi z $\text{REAL LIFE}^{TM}$.
	\item To oznacza, że ilość elementów różnych od $0$ jest rzędu $O(n)$. Mówimy wtedy że macierz jest rzadka.
	\item Wniosek: Jeśli $ A $ ma $ O(n) $ niezerowych elementów, to mnożenie $Ax$ kosztuje też $ O(n) $. Ponadto, rozwiązanie układu z macierzą dolnotrójkątną też jest $ O(n) $

\end{itemize}
\section{Normy macierzowe i wektorowe}
\paragraph{Normy wektorowe}
$$ ||x||_{p}:=(\sum_{i=1}^{N}|x_i|^p)^\frac1p $$
$$ ||x||_\infty:=\max_i |x_i| $$
\paragraph{Norma macierzowa}
$$ ||A||_p := \max_{x\not=0}\frac{||Ax||_p}{||x||_p}=\max_{||x||_p}||Ax||_p $$
\paragraph{Własności normy macierzowej}
\begin{enumerate}
	\item $$||Ax||\leq||A||\dotsm||x|| \forall_{x\in\mathbb{R}^n}$$
	\item $$ ||Ax|| $$ - nie dało się przeczytać tablicy
	\item tu też coś było :(
\end{enumerate}

\section{Warunek wystarczający zbieżności klasycznej metody iteracyjnej ($A=M-Z$)}
\begin{equation}
Mx_{k+1}=b+Zx_k\tag{$*$}
\end{equation}
Niech $x^*$ będzie dokładnym rozwiązaniem $Ax^*=b$
$$x_{k+1}=M^{-1}(b-Zx_k)$$
$$x_{k+1}-x^*=M^{-1}(b-Zx_k)-x^*$$
$$=M^{-1}(Ax^*-Zx_k)-x^*$$
$$=M^{-1}(Ax^*-(M-A)x_k)-x^*$$
$$=M^{-1}Ax^*+(I-M^{-1}A)x_k-x^*$$
$$=-(I-M^{-1}A)x^*+(I-M^{-1}A)x_k$$
$$=(I-M^{-1}A)(x_k-x^*)$$
Czyli $B$ pomnożony błąd $k$-ty.\\
Czyli $ x_{n+1}-x^*=B(x_k-x^*)=B^2(x_{k-1}-x^*)\ldots=B^{k+1}(x_0-x^*) $

\paragraph{Wniosek:} Jeśli $ ||B||<1 $, to $(*)$ zbieżna do $x*$ dla dow. $x_0\in \mathbb{R}^N$

\paragraph{Twierdzenie:} Metoda $ (*) $ jest zbieżna do $ x^* $ z dowolnego $ x_0 $ wtw gdy $ \rho(B)<1 $
 gdzie $ \rho(B)=\max\{|\lambda|:\lambda \text{ jest wartością własną }B\} $ - promień spektralny macierzy $ B $
 Dowód pominięty

\paragraph{Twierdzenie:} Jeśli macierz $ A $ jest ściśle diagonalnie dominująca, tzn zachodzi
$ |a_n|>\sum_{j\not=i}|a_{ij}|\text{ dla }i=1..N $
 to metoda Jacobiego jest zbieżna (dla dowolnych $ x_n\in R^n $)
\begin{proof}
	Zbadajmy macierz iteracji.
	$$ ||B||_\infty = ||I-M^{-1}A||_\infty $$
	$ M^-1 $ dla macierzy diagonalnej to podnoszenie wszystkich elementów do $ -1 $.\\
	$ M^{-1}A=I $ + macierz z zerami na diagonali i ułamkami na reszcie, pierwszy wiersz to $ 0, a_{12}/a_{11}, a_{13}/a_{11}\ldots $\\
	Żeby uzyskać $ B $ odejmujemy $I$.\\
	$ ||B||_\infty = \max_i w_i  $\\
	$ w_i = \sum_j |b_{i,j}|=\sum_{j\not=i}|a_{ij}/a_{ji}|=\frac1{|a_{ii}|}\sum_{j\not=i}|a_{ij}|<1 $
	zatem norma $ B $ jest mniejsza od 1 więc normy są zbieżne.
\end{proof}

\section{Metody iteracyjne oparte na normalizacji w przestrzeni Kryłowa}
\paragraph{$k$-ta przestrzeń Kryłowa}
$$ K_k = {r_0, Ar_0, ...,A^{k-1}  r_0} $$
gdzie $ r_k:=b-Ax_k $ - reszta na $ k $-tej iteracji
\paragraph{Metoda iteracyjna}
\begin{itemize}
	\item $ x_k+1 \in K_k $ przesunięta o $ x_0 $
	\item $ x_k+1 $ normalizuje pewną miarę błędu na $ x_0+K_k $
	\item Na przykład: $$ ||x_k-X^*||_C\leq ||y-x^*||_C \forall_{y\in x_0+K_k} $$
	lub $$ ||r_k||\leq ||b-Ay||_C \forall_{y\in x_0+K_k} $$
	gdzie $ C =C^T>0 $
\end{itemize}

\subsection{Metoda gradientów sprzężonych (CG - Conjugate Gradient) dla macierzy $A=A^T>0$}
\subsubsection{Fakty o macierzach symetrycznych i dodatnio określonych}

Niech $A = A^T>0$ (symetryczna i dodatnio określona, a co za tym idzie $x^TAx>0\text{ dla }x\not=0$). Wtedy:
\begin{enumerate}
	\item Wartości własne są rzeczywiste a wektory własne są ortogonalne (czyli $ A=Q\Lambda Q^T $, gdzie $ Q $ jest ortogonalna, a $ \Lambda $ jest diagonalna)
	\item $ ||x||_A:=\sqrt{x^TAx} $ określa normę wektorową (norma energetyczna indukowana przez $ A $)
\end{enumerate}
Iterację metody gradientów sprzężonych definiujemy następująco:
$$ x_{k+1}\in x_0 + K_k $$
$$ ||x_{k+1}-x^*||_A\leq ||y-x^*||_A\forall_{y\in x_0+K_k} $$
Ale przecież potrzebujemy mieć rozwiązanie żeby to zrobić!
\paragraph{Fakt.} Można stąd wyprowadzić algorytm iteracyjny, który na podstawie kilku poprzednio wyznaczonych wektorów wyznaczy $ x_{k+1} $ kosztem jednego mnożenia przez macierz $ A $ i $ O(N) $
\paragraph{Twierdzenie.} Po $ k $ iteracjach metody CG błąd $ ||x_k-x^*||_A \leq 2(\frac{\sqrt{\alpha}-1}{\sqrt{\alpha}+1})^k ||x_0-x^*||_A $
gdzie $ \alpha = \lambda_{max}(A)/\lambda_{min}(A) $.

\section{Zagadnienia własne}

Dla $ A \in R^{NxN} $ znaleźć parę własną $ (\lambda, x)$, że $Ax = \lambda x$ oraz $x \neq 0$.
$\lambda$ pierwiastkiem wielomianu charakterystycznego: $det(A - \lambda I) = 0$
Gdy $A = A^T$ to wartości i wektory własne rzeczywiste, istnieje $Q$ ortogonalna $A = Q*L*Q^T$ (L to tylko lambdy na przekątnej)


3 podstawowe klasy zadań obliczeniowych dla zagadnień własnych:

1. ekstremalne wartości własne (największa, najmniejsza, etc) i odp. wektory (PageRank)

2. wartości własne bliskie zadanej wartości (wieżowce w Japonii)

3. pełne zadanie własne

Wyznaczanie wektora odpowiadającego dominującej wartości własnej (zakładamy że istnieje dokładnie jedna wartość własna że jej moduł ostro większy od innych modułów)

$$\|A_x\| = \|\lambda*x\| = |\lambda| * \|x\| = |\lambda| $$ (bo $\|x\| = 1$)

Metoda potęgowa
$x_0$ startowy o normie 1
$$x_{n+1} = A  x_n$$
$$x_{n+1} := \frac{x_{n+1}}{\|x_{n+1}\|} $$
skąd nazwa:
$$x_{n+1} = Ax_n = AAx_{n-1} = A^2x_{n-1} = \dots = A^{n+1}x_0 $$
nie robić tego w ten sposób, bo $A$ jest duże (ale rzadkie) i będzie coraz mniej rzadkie!
Lepiej iteracyjnie, bo tanio mnożyć przez rzadką macierz

Twierdzenie o zbieżności tej metody:
Załóżmy, że A diagonalizowalna - istnieje Y nieosobliwe że
$YAY^{-1}$ tworzy macierz diagonalną

$Ay_i = \lambda y_i$ gdzie $y_i$ to kolumna $Y$

$$ x_0 = \sum_{1}^{n} \alpha_i  y_i $$
$$x_n = A^nx_0 = A^{n-1}(Ax_0) = $$
$$ = A^{n-1}\sum_{1}^{n} \alpha_i y_i = $$
$$ = A^{n-1}\sum_{1}^{n} \alpha_i \lambda_i y_i = $$
$$= \sum_{1}^{n} \alpha_i \lambda_i^n y_i = $$
$$= \lambda_1^n *\sum_{1}^{n} \alpha_i (\frac{\lambda_i}{\lambda_1})^n y_i$$

Jeżeli $\lambda_1$ dominujące, to $\frac{\lambda_i}{\lambda_1^n} \rightarrow 0 $
($\lambda_1 \neq 0 $)

Odzyskanie wartości własnej na podstawie przybliżenia (znaleźć takie przybliżenie lambdy że norma przybliżenia $A*x-\lambda*x$  minimalna) - jest to zadanie najmniejszych kwadratów
iloraz Rayleigh TODO
Transformacje spektrum:
1. Jeżeli $\lambda$ ww $A$ to $\lambda-\mu$ ww $A-\mu*I$
2. Jeżeli $\lambda$ ww $A$ nieosobliwego to $1/(\lambda)$ ww $A^(-1)$

Odwrotna metoda potęgowa na zadania typu 2:

Wartosci wlasne $(A - \mu*I)^{-1}$ to $\frac1{\lambda-\mu} $
Kiedy największe? Kiedy $\mu$ blisko $\lambda_i$ to wtedy $\frac{1}{\lambda_i - \mu}$ dominującą ww

RQI raileigh quotient iteration, bardzo szybko zbieżne ale niekoniecznie do najbliższego oryginałowi ww TODO

tak naprawdę metoda potęgowa nie na jednym wektorze a na wszystkich, zazwyczaj słabo działa, modyfikacja "raz dodajemy a raz odejmujemy"


3. pełny problem - metoda QR \\
Skorzystamy z następujących faktów z GAL-u:
\begin{itemize}
	\item Macierze $ A, B $ są podobne jeśli istnieje macierz M spełniająca $ A=MBM^{-1} $
	\item Ortogonalna macierz $ Q $ spełnia $ Q^{-1}=Q^T $
	\item Macierze podobne mają te same wartości własne
	\item W macierzy trójkątnej wartości własne są na przekątnej
\end{itemize}
Zatem jeśli sprowadzimy macierz $A$ do podobnej do niej macierzy trójkątnej, to będziemy znać wszystkie wartości własne $A$. Można wykorzystać do tego rozkład $QR$. Definiujemy metodę iteracyjną:
$$A_{k+1}=R_kQ_k, \text{ gdzie } A_k=Q_kR_k$$
Aby nie wyznaczać całego rozkładu można rozpisać to:
$$A_{k+1}=R_kQ_k=Q_k^TA_kQ_k$$
Aby koszt pojedynczej iteracji był niższy, należy na początek użyć Householdera w celu sprowadzenia macierzy $ A $ do postaci Hessenberga (macierz która może mieć niezerowe elementy tam gdzie może albo trójdiagonalna albo górnotrójkątna).
\section{Wykład 5 - Arytmetyka zmiennoprzecinkowa}
\subsection{Notacja wykładnicza}
Np. $6.63\times 10^{-34}$ jest przybliżeniem stałej Plancka.
U nas będzie tak:
$$x=-1^s m\beta^{e}$$
gdzie:
\begin{enumerate}
	\item $\beta$ - podstawa (typowo $ \beta =2$)
	\item $ e $ - wykładnik spełniający $ e_{min}\leq e\le e_{max} $
	\item $ m $ - mantysa, $ 1\le m < \beta $
	\item $ s\in\{0,1\} $ - znak liczby
\end{enumerate}
Tak można zapisać każdą liczbę rzeczywistą. Jak to zrobić żeby zmieściło się? Zakładamy, że $ m=(f_0\cdot f_1\cdot f_2\cdot f_3\cdot \ldots\cdot f_{p-1})_\beta $, gdzie $ f_i\in\{0,1,\ldots, \beta-1\} $ oraz $ \_\cdot\_ $ jest konkatenacją. To oznacza, że m jest liczbą w systemie o podstawie $ \beta $.

Zatem:
$$x=-1^s\cdot(\sum_{i=0}^{p-1}f_i\beta^{-i})\beta^e$$
Zakres wykładników określa nam 
\subsection{Liczby znormalizowane (maszynowe)}
$$ m=(1\cdot f_1\cdot f_2\cdot f_3\cdot \ldots\cdot f_{p-1})_2 $$
To nam daje $ 1\le m<2 $.
\paragraph{Reprezentacja liczby maszynowej w pamięci}
Musi być za pomocą sekwencji bitów. Jak?
Do $e$ dodajemy $bias$, żeby nie musieć pamiętać znaku.
$$\begin{array}{|c|ccc|ccccc|}
s&&e+bias&&f_1&f_2&f_3&\ldots&f_{p-1}
\end{array}$$
$ e + bias $ będzie zatem miało $ size - p $ bitów.
\subsection{Standard arytmetyki zmiennoprzecinkowej}
\paragraph{IEEE-754} - (pierwsza wersja - 1985, najnowsza 2008)
definiuje następujące typy (dla $ \beta =2 $):
$$\begin{bmatrix}
\text{Nazwa potoczna} & p&e_{min}&e_{max}&size&bias&\text{Oficjalna nazwa}\\
\text{half precision}&11&-14&15&16&15&\text{binary16}\\
\text{single precision}&24&-126&127&32&127&\text{binary32}\\
\text{double precision}&53&-1022&1023&64&1023&\text{binary64}\\
\text{quad precision}&113&-16382&16383&128&16383&\text{binary128}\\
\text{double extended precision}&64&-16382&16383&80&16383&\text{IA-32}\\
\end{bmatrix}$$
Kiedy dodamy $ bias $ do $ e_{min} $ dostajemy 1, a nie 0!
Zarezerwowane wartości $ e+bias $ to $"0"^n$ oraz $ "1"^n $.
$$
\begin{bmatrix}
\text{Liczba}&\text{Znak}&e+bias&\text{mantysa } (f_1f_2f_3\ldots f_{p-1})\\
+0&0&0\ldots0&0\ldots0\\
-0&1&0\ldots0&0\ldots0\\
+\infty&0&1\ldots1&0\ldots0\\
-\infty&1&1\ldots1&0\ldots0\\
NaN&0&1...1&\{0,1\}^*1\{0,1\}^*
\end{bmatrix}
$$
A skąd te liczby? $ \frac{1}{+0} = \infty $, $\frac{1}{-0}=-\infty$. Co za tym idzie, $ \frac1\infty =0$. A skąd $ NaN $? $ \frac00 $, $ \infty-\infty $ itd.

\paragraph{Orientacyjne zakresy liczb znormalizowanych:}
$$\begin{matrix}
&\text{najmniejsza}&\text{największa}\\
\text{binary32}&\sim10^{-38}&\sim10^{38}\\
\text{binary64}&\sim10^{-308}&\sim10^{308}\\
\end{matrix}$$

\paragraph{Bardzo mały system liczb maszynowych}
$$ \beta=2, p=3, e_{min}=-1, e_{max} =2$$
Liczby znormalizowane są postaci:
$$ x=-1^s\cdot(1f_1f_2)_22^e $$
$e=0$:\\
1, 1.25, 1.5, 1.75\\
$e=1$:\\
2, 2.5, 3, 3.5\\
$e=2$:\\
4, 5, 6, 7, 8\\
$e=-1$:\\
0.5, 0.625, 0.75, 0.875\\

Ale nie ma nic między 0 i 0.5!
\paragraph{Liczby zdenormalizowane}
$$x=-1^s(0f_1f_2\ldots f_{p-1})2^{e_{min}}$$
Wypełniają pustkę wokół zera.
$$
\begin{matrix}
\text{liczba}&\text{znak}&e+bias&\text{mantysa}\\
\text{+subnormal}&0&\ldots&\{0,1\}^*1\{0,1\}^*\\
\text{-subnormal}&1&\ldots&\{0,1\}^*1\{0,1\}^*\\
\end{matrix}
$$
Co w naszym małym systemie daje dodatkowe 3 liczby wokół 0.
\subsection{Działania arytmetyczne na liczbach IEEE-754}
\begin{enumerate}
	\item Reprezentacja liczby rzeczywistej $x: fl(x)$ - najbliższa $ x $ w sensie zaokrąglenia do najbliższej (default, da się zmienić by było do zera lub od zera)
	\item Jeśli $ x $ jest reprezentowana przez znormalizowaną l. maszynową, to zachodzi $ \frac{|fl(x)-x|}{|x|}\le2^{-p} $, gdzie $ p $ jest ilością bitów w mantysie - $ \nu $ - precyzja arytmetyki. Inaczej mówiąc, $ fl(x)=x(1+\varepsilon), |\varepsilon|\le\nu $
	\item Dla $ \cdot\in\{+,-,\times,\div\} $ standard wymaga, by: 
	$$ \begin{matrix}
	fl(a\cdot b) & = & fl(a\cdot b)\\
	\begin{matrix}
	\text{wynik obliczenia }a\cdot b\text{ w arytmetyce }fl\\
	\text{ gdy } a,b \text{ są liczbami maszynowymi} 
	\end{matrix}
	
	& & 
	\begin{matrix}
		\text{reprezentacja dokładnego}\\
		\text{ wyniku } a\cdot b\text{ w arytmetyce }fl
	\end{matrix}
\end{matrix}$$
\item Zatem możemy uznać, że $ fl(a\cdot b) = (a\cdot b)(1+\varepsilon), |\varepsilon|\le \nu$, z tym że $ \varepsilon $ jest inny dla różnych działań, oraz pod warunkiem, że wynik jest reprezentowalny.
\end{enumerate}
\section{Wykład 6 - 15/11}
Na kolokwium nie będzie arytmetyki zmiennoprzecinkowej ani dzisiejszego wykładu.
\subsection{Jeszcze trochę $fl$ - a}
Arytmetyka zmiennopozycyjna nie jest zwykłą arytmetyką.
\begin{itemize}
\item Nieprawdą jest że $ \sim(a==b) \iff a\text{ nie jest } b $, bo NaN jest nieporównywalny
\item nie musi być, że jeśli $ 1 + x == 1 $ to $ x = 0 $. (np. dla $ x= $ precyzja arytmetyki)
\item unikać testów z równością
\item wynik $ x - x + 1 $ może być różny w zależności od kolejności działań.
\item stawiać nawiasy by wymusić kolejność
\item kumulacja błędów zaokrągleń w obliczeniach 
\item wzmocnienie wcześniejszych błędów może być katastrofalne

\end{itemize}
\paragraph{Przykład (redukcja cyfr przy odejmowaniu)}. Obliczmy $ s:=x+y, x,y\in\mathbb{R} $. Ale zamiast $ x, y $ w komputerze mamy tylko $\tilde{x}= fl(x) = x(1+\varepsilon_x), \tilde{y}=fl(y)=y(1+\varepsilon_y) $!
Zamiast $ s $ obliczymy w $ fl $ wartość $ \tilde{s}=fl(\tilde{x}+\tilde{y})= (\tilde{x}+\tilde{y})(1+\delta)$.
Zatem błąd wyniku to $ \frac{|s-\tilde{s}|}{|s|} = \frac{|x+y-fl(\tilde{x}+\tilde{y})(1+\delta)|}{|x+y|}=x(1+\varepsilon_x)(1+\delta)+ y(1+\varepsilon_y)(1+\delta)=x(1+E_x)+y(1+E_y)$ gdzie $ E_x=\varepsilon_x+\delta+\varepsilon_x\delta $
Zatem $ |E_x|\le|\varepsilon_x|+|\delta|+ |\varepsilon_x\delta|\le2ni+ni^2\approx2ni $
Zatem błąd =$ \frac{|s-\tilde{s}|}{|s|} =\frac{|x+y-x(1+E_x)-y(1+E_y)|}{|x+y|}=\frac{|xE_x-yE_y|}{|x+y|}\le2\frac{|x|+|y|}{|x+y|}ni$
Zauważmy, że:
\begin{itemize}
\item jeśli $ x, y $ - tego samego znaku, to $ \frac{|x|+|y|}{|x+y|}=1 $ i wtedy błąd szacuje się przez $ 2ni $
\item jeśli $ x\approx y $ to $ \frac{|x|+|y|}{|x+y|} >> 1 $ i wtedy błąd ograniczamy przez bardzo dużą liczbę
\end{itemize}
\subsection{Katastrofy spowodowane $ fl $ - em}
\begin{itemize}
\item zatonięcie całej platformy wiertniczej spowodowanej niewłaściwym użyciem pakietu numerycznego.
\item Rakiety patriot - im dłużej stały włączone, tym więcej celności traciły. Systemowy zegar liczył ticki co $ \frac1{10} $ sekundy. Więc $t * \frac1{10}$ daje tym gorszy błąd im więcej ticków.
\item rakiety arian - double został przepisany na short inta
\item rakieta nie poleciała na marsa bo system imperialny vs. metryczny
\item partia zielonych w niemczech dostała 4.76\% głosów ale excel zaokrąglił w górę i przeszli próg wyborczy
\item indeks giełdowy jechał w dół a powinien był w górę, to przez częstą aktualizację z błędem

\end{itemize}
\subsection{Błędy w obliczeniach numerycznych}
Mamy zadanie obliczeniowe, algorytm i dane. Niech $ P_i: \mathbb{R}^n\supseteq D\rightarrow\mathbb{R}^m $ \\
Problem obliczeniowy: mając dane $ x\in D $, wyznacz $y=P(x)$.\\
\paragraph{Przykłady}
\begin{itemize}
\item Dla zadanej funkcji rzeczywistej oblicz $ y=f(x) $
\item dla zadanych $ A\in\mathbb{R}^{n\times n} $ oblicz $ y=Ax $ ($ P=Ax $)
\item dla $ A\in\mathbb{R}^{n\times n} $ nieosobliwej oraz $ b\in\mathbb{R}^n $ oblicz $ b\in\mathbb{R}^n $ t. że $ Ay=b $ ($P=A^{-1}x $)
\item dla $ f: \mathbb{R}\rightarrow\mathbb{R}$ oblicz $ y $ t. że $f(y)=0$ ($P(x)=f^{-1}(x)  $)
\end{itemize}
Zwykle szukamy nie algorytmu dla danego zadania, tylko klasy zadań.
\paragraph{Definicja}
Błąd bezwzględny: $ \|x-\tilde{x}\| $.
Błąd względny: $ \frac{\|x-\tilde{x}\|}{\|x\|} (x\not=0)$.
Nieuniknione w $ fl $ są błąd reprezentacji danych oraz wyników
Algorytm jest silnie numerycznie poprawny (backward stable) jeśli wynik jego działania w fl można zinterpretować jako wynik zadania obliczeniowego (w arytmetyce dokładnej ) na danych lekko zaburzonych.\\
Algorytm jest numerycznie poprawny, jeśli wynik też może być zaburzony na poziomie reprezentacji.
\paragraph{Uwarunkowanie zadania numerycznego}
Jak zaburzenie danych wpływa na zaburzenie algorytmu?
$$cond_{abs}(P,x):=\sup_{\Delta\not=0, \|\Delta\|<\delta}\frac{\|P(x+\Delta)-P(x)\|}{\|\Delta\|}$$

- uwarunkowanie zadania P w punkcie x\\
Innymi słowy, $ \|P(x+\Delta)-P(x)\|\le cond_{abs}(P,x)\|\Delta\| $, gdzie $ \Delta $ jest najmniejsza możliwa dla danych zaburzeń.
Można to idealizować i rozważać przypadek graniczny, gdy $ \|\Delta\|\rightarrow0 $
$$cond_{abs}(P, x):=\lim_{\|\Delta\|\rightarrow0}\frac{\|P(x+\Delta)-P(x)\|}{\|\Delta\|}$$
Analogicznie, uwarunkowanie względnie: 
$$\frac{\|P(x+\Delta)-P(x)\|}{\|P(x)\|}\le cond_{rel}(P, x)\frac{\|\Delta\|}{\|x\|}$$
\paragraph{Przykład}
\begin{itemize}
\item $ P(x)= f(x) $:  $ cond_{abs}(P, x)=|f'(x)| $
\item $ Ax=b, A\tilde x=\tilde b$
\end{itemize}
Jak błąd wzgl. $x$ zależy od błędu wzgl. $b$?
niech błąd względny $b \le \varepsilon$, wtedy 

$$\tilde{x} = A^{-1}\tilde{b}=A^{-1}(b+\Delta)=A^{-1}b+A^{-1}\Delta$$
zatem $\|x-\tilde{x}\|\le\|A^{-1}\Delta\|\le\|A^{-1}\|\|\Delta\|$
stąd $$\text{błąd}  \le  \|A^{-1}\|\frac{\|\Delta\|}{\|b\|}\frac{\|b\|}{\|x\|}<=\|A^{-1}\| \varepsilon \frac{\|Ax\|}{\|x\|} <= \|A^{-1}\|\|A\||\varepsilon =cond(A)varepsilon$$
Można pokazać ogólniej, że jeśli $ Ax=b $ oraz $ \tilde{A}\tilde{x}=\tilde{b} $, gdzie zaburzenia wzgl. $ b $ i $ A $ są na poziomie dostatecznie małego epsilona, to błąd $ x $ da się oszacować przez $ 4cond(A)\varepsilon $.



\part{Ćwiczenia}
\section{Układy nadokreślone - kontynuacja}
\subsection{Zadanie 1.}
\paragraph{Macierz Hessenberga} - to macierz trójkątna górna, z tym że niezerowe elementy mogą być jeden element pod diagonalą.
$$\begin{bmatrix}
	x&\ldots&&&&x\\
	x&x&\ldots&&&x\\
	&x&x&\ldots&&x\\
	&&\ddots&&&x\\
	&&&x&x&x\\
	&&&&x&x\\
\end{bmatrix}$$
Jak najmniejszym kosztem znaleźć rozkład $ QR $ tej macierzy?\\
Metodą Householdera? Nie ma jak wykorzystać zer na dole.
\subsubsection{Obroty Givensa - przypomnienie}
\begin{enumerate}
	\item $ G_{ij} $ - macierz Givensa
	\item $ b=G_{ij}a$
	\item $ b_j=0 $
	\item $ \cos\phi = \frac{a_i}{\sqrt{a_i^2+a_j^2}} $
	\item $ \sin\phi = \frac{a_j}{\sqrt{a_i^2+a_j^2}} $
\end{enumerate}

\subsubsection{Zamiana macierzy Hessenberga w górnotrójkątną obrotami Givensa}
$$ (G_{ij}a)_j = -\frac{a_ia_j}{\sqrt{a_i^2 + a_j^2}}+\frac{a_ia_j}{\sqrt{a_i^2 + a_j^2}}=0$$
$$G_{n-1 n}\ldots G_{i i+1}\ldots G_{12}A=R$$
\subsubsection{Koszt}
Robimy $ n-1 $ iteracji.\\
Dla $ G_{i\ i+1} $ trzeba wykonać jeden pierwiastek, $ w_i =cw_i+sw_{i+1}$ oraz $w_{i+1} = -sw_i + cw_{i+1}$, łącznie $4(n-1) $ mnożeń.\\
Wszystko razem: $4\sum_{i=1}^{n-1}n-i=4\sum_{i=1}^{n-1}i=\frac{4n(n-1)}{2}\sim2n^2$.
\subsection{Zadanie 2.}
Dane są punkty $(-1, -1), (0,2), (1, 0), (2, 1)$.\\
Znajdź prostą $ y=ax+b $ najlepiej przybliżającą te punkty (w sensie LZNK).\\
Zadane punkty oznaczamy jako $ (x_i, y_i) $.\\
Zatem to, co chcemy zminimalizować to $y(x_i)-y_i$.\\
Policzmy normę: $ \min_{a, b} \sum_{i=1}^{4}(y(x_i)-y_i)^2$\\
Niewiadome to $ a $ oraz $ b $, więc niech:
$$
\begin{matrix}
	z=\begin{bmatrix}a\\b\end{bmatrix}&
	d=\begin{bmatrix}y_i\end{bmatrix}_{i=1, 2, 3, 4}&
	A=\begin{bmatrix}
		1&x_1\\
		1&x_2\\
		1&x_3\\
		1&x_4\\
	\end{bmatrix}
\end{matrix}
$$
Teraz wystarczy użyć LZNK aby obliczyć $\min{||Az-d||_2}$:
\paragraph{TODO:} policzyć to
\paragraph{Uwaga:} w ten sposób odległość między punktami a prostą liczymy w pionie, a nie najbliższą (to dobrze, tak działą LZNK).
\paragraph{Uwaga 2:} LZNK nie działa dla równania $ y=a+e^{bx} $, ale dla $ y=a+be^x $ już tak!
\section{Normy}
\paragraph{Przypomnienie definicji: } Norma $ ||\cdot||: V\rightarrow\mathbb{R}^+$ spełnia następujące warunki:
\begin{enumerate}
	\item $||u+v||\leq||u||+||v||$
	\item $||\alpha v||=||av||$
	\item $||v||=0\implies v=0$ - wektor zerowy
\end{enumerate}
\paragraph{$p$-te normy wektorowe:}
$$||x||_p = \sqrt[p]{\sum_{i=1}^{n}|x_i|^p}$$
$$||x||_\infty = \max_i |x_i|$$
\paragraph{Normy macierzowe}
Niech $A\in \mathbb{R}^{n\times n}$.\\
Macierzowe normy indukowane są postaci
$$||A||=\sup_{x\not=0}\frac{||Ax||}{||x||}=\sup_{||x||=1}||Ax||$$
\subparagraph{$p$-te normy macierzowe}
$$||A||_p=\sup_{||x||_p=1}||Ax||_p, p=1, 2, \ldots, \infty$$
wszystkie poza $1, 2, \infty$ zwykle się pomija
\subsection{Własności norm indukowanych macierzy}
\begin{enumerate}
	\item $||Ax||\leq||A||||x||$ - z definicji mamy $ ||A||\geq\frac{||Ax||}{||x||} $
	\item $ ||AB||\leq||A||||B||, A,B\in \mathbb{R}^{n\times n} $ - bo
	$$ ||ABx||\leq||A||||Bx||\leq||A||||B||||x|| $$ oraz
	$$ ||AB||=\sup_{x\not=0}\frac{||ABx||}{||x||}\leq||A||||B|| $$
\end{enumerate}
\paragraph{Fakt.} W przestrzeniach skończonego wymiaru wszystkie normy spełniają równanie:
$ \exists_{c_1, c_2>0}\forall_{x}c_1||x||_1\leq||x||2\leq c_2||x||_1 $, gdzie normy są dowolne (niekoniecznie pierwsza i druga)

\subsection{Zależności między normami}
Niech $ x\in\mathbb{R}^n $, a normy będą $p$-te.

$$ ||x||^2_1=(\sum_i|x_i|)^2 \ge ||x||_2^2$$
$$ ||x||_1\le \alpha||x||_2 $$
Jakie $\alpha$ wybrać?
$$ ||x||_1\ge ||x||_\infty $$
$$ n||x||_\infty\ge ||x||_1 $$
$$ ||x||_\infty\le ||x||_2 $$
$$ \sqrt n||x||_\infty\ge ||x||_2 $$
$$ ||x||_1\leq n||x||_\infty\leq n||x||_2 $$
Zatem $\alpha=n$.
\paragraph{Nierówności}
$ \frac1n ||A||_2\le\frac1{\sqrt n}||A||_\infty\le||A||_2\le\sqrt{n} ||A||_1\le n||A||_2$
\subsection{Wzory na normy macierzowe}
$$ ||A||_1=\max_j\sum_{i=1}^{n} |a_{ij}|$$
$$ ||A||_\infty=\max_i\sum_{j=1}^{n} |a_{ij}|$$
Zatem $ ||A^T||_1=||A||_\infty $.\\
\paragraph{Norma druga (spektralna)}
$$||A||_2=\max_{\lambda\in\delta(A^TA)}\sqrt{\lambda}$$
Gdzie $ \delta(M) $ jest zbiorem wartości własnych macierzy $ M $.\\
Jeśli Q ortogonalna: $||Q||_2=1$, co za tym idzie $ ||I||_2=1 $
$ \lambda $- wartość własna oraz $ v $ - wektor własny spełnieją $ Av=\lambda v $
\paragraph{Norma Frobeniusa (Euklidesowa)}
$$ ||A||_F = \sqrt{\sum_{i,j}|a_{ij}|} $$
Nie jest normą indukowaną, bo dla wszystkich norm indukowanych zachodzi $ ||I||=\sup_{x\not=0}\frac{||Ix||}{||x||}=1 $, a
$ ||I||_F=\sqrt{n} $ - zatem nie pochodzi od drugiej normy wektorowej!

\section{Ćwiczenia 26/10}
\subsection{Dalsze własności norm}
$$ ||A||_2=\max_{\lambda\in\delta(A^TA)}\sqrt{\lambda} $$
Jeśli $ A=A^T $ to $ ||A||_2=\max_{\lambda\in\delta(A)}|\lambda| $. 
\paragraph{Twierdzenie.} Jeśli $ \lambda $ jest wartością własną $ A $, to $ \lambda^2 $ jest wartością własną $ A^2 $.\begin{proof}
	$Av=\lambda v \implies A^2v=\lambda Av=\lambda^2v$
\end{proof}
$$ ||A||_2=\sup_{||x||=1}||Ax||_2 $$
$$||Ax||_2=\sqrt{(Ax)^TAx}=\sqrt{x^TA^TAx}=(*)$$
$ A^TA $ jest macierzą symetryczną, oraz $ A^TA=Q^T\Lambda Q $, gdzie $ \Lambda $ jest macierzą diagonalną $ \Lambda=\text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_n) $ oraz $ Q $ jest macierzą ortogonalną wektorów własnych.

\paragraph{UZUPEŁNIĆ TODO}
\section{Ćwiczenia 9/11}
\subsection{Metoda Richardsona - kontynuacja zadania}
Metoda Richardsona - $ x_{k+1}=x_k+\tau(b-Ax_k) $
Szukaliśmy parametru $ \tau $ t. że metoda Richardsona jest zbieżna.
$ A $ ma wartości własne $ \lambda_1, \ldots, \lambda_n>0 $.
Jest zbieżna dla $ \tau\in(0, \frac2{\lambda_{max}}) $. Dla jakich $ \tau $ jest zbieżna najszybciej?
$$\rho(I-\tau A)=\max\{|1-\tau\lambda_{min}|, |1-\tau\lambda_{max}|\}$$
Powiedzieliśmy, że
$$ ||x_{k+1}=x^*||\le||I-Q^{-1}A||||x_k-x^*|| $$
Zatem szukamy $ \tau $ realizującego: 
$$ {\arg\min}_{\tau\in(0, \frac2{\lambda_{max}})}\rho(I-\tau A)$$
Czyli: 
$$ {\arg\min}_{\tau\in(0, \frac2{\lambda_{max}})} \max\{|1-\tau\lambda_{min}|, |1-\tau\lambda_{max}|\} $$

Pierwsz funkcja ma miejsce zerowe w $ \frac1{\lambda_{min}} $, a druga w $ \frac1{\lambda_{max}} $. Można narysować obie funkcje, one przecinają się w zwykle dwóch punktach, czyli $ |1-\tau\lambda_{min}| = |1-\tau\lambda_{max}| $. Zatem albo $ \tau = 0 $, co daje nam rozbieżność, albo $ \tau=\frac2{\lambda_{max}+\lambda_{min}} $ Żeby wystartować z metodą, to wystarczyłoby ograniczenie górne na $ \lambda_{max} $
\subsection{Metoda Gaussa-Seidela}
$$A=\begin{bmatrix}
	2&-1&&&&\\
	-1&2&-1&&&&\\
	&-1&2&-1&&&\\
	&&\ddots&\ddots&\ddots&&\\
	&&&\ddots&\ddots&\ddots&\\
	&&&&-1&2&-1\\
	&&&&&-1&2
\end{bmatrix}$$
Wykaż, że metoda Gaussa-Seidela jest zbieżna dla macierzy A.
\paragraph{Fakt.} Metoda Gaussa-Seidela jest zbieżna dla macierzy diagonalnie dominującej $ (\forall_{i}|a_{ii}|>\sum_{j\not=i}|a_{ij}|) $.
Metoda iteracyjna jest zbieżna, jeśli promień spektralny macierzy jest mniejszy od 1.
\paragraph{Promień spektralny} 
\begin{itemize}
	\item$$ \rho(I-Q^{-1}A) $$
	\item kres dolny po wszystkich normach indukowanych:
	$$\inf_{||\cdot||\text{jest indukowana}}||I-Q^{-1}A ||$$
\end{itemize}

Wystarczy pokazać, że dla pewnej normy indukowanej $ ||\cdot|| $(jakiej?) zachodzi: $$ ||I-Q^{-1}A ||<1$$
\begin{proof}
Tutaj, $$
Q=\begin{bmatrix}
2&&&&&\\
-1&2&&&&&\\
&-1&2&&&&\\
&&\ddots&\ddots&&&\\
&&&\ddots&\ddots&&\\
&&&&-1&2&\\
&&&&&-1&2
\end{bmatrix}
$$
Korzystamy z tego, że $ Q^{-1}Q=I $, i prostym spostrzeżeniem jest:
$$
Q=\begin{bmatrix}
2^{-1}&&&&&\\
2^{-2}&2^{-1}&&&&&\\
\vdots&&2^{-1}&&&&\\
\vdots&&\ddots&\ddots&&&\\
\vdots&&&\ddots&\ddots&&\\
\vdots&&&&&2^{-1}&\\
2^{-n}&&&&&2^{-2}&2^{-1}
\end{bmatrix}
$$

Teraz 
$$
Q^{-1}A=\begin{bmatrix}
1&-\frac12&&&&\\
0&\frac34&-\frac12&&&&\\
&-2^{-3}&\frac34&-\frac12&&&\\
&\vdots&\ddots&\ddots&\ddots&&\\
&&&\ddots&\ddots&\ddots&\\
&&&&\ddots&\frac34&-\frac12\\
0&-2^{-n}&\ldots&\ldots&&-2^{-3}&\frac34
\end{bmatrix}
$$
$$
G=I-Q^{-1}A=\begin{bmatrix}
1&\frac12&&&&\\
0&\frac14&\frac12&&&&\\
&2^{-3}&\frac14&\frac12&&&\\
&\vdots&\ddots&\ddots&\ddots&&\\
&&&\ddots&\ddots&\ddots&\\
&&&&\ddots&\frac14&\frac12\\
0&2^{-n}&\ldots&\ldots&&2^{-3}&\frac14
\end{bmatrix}
$$
$ ||G||_1=\sum_{i=1}^n(\frac12)^i=1-\frac1{2^n}<1 $
\end{proof}
\subsection{Błędy numerycznych rozwiązań układów równań}
Rozwiązujemy układ $ Ax^*=b $, $ x^* $ jest dokładnym rozwiązaniem, $ x $ - wynik obliczeń numerycznych. Wtedy $ x^*=x+A^{-1}(b-Ax)=x+A^{-1}r=x+e $, gdzie $ r $ jest wektorem residualnym ($ r=b-Ax $), a $ e $ - błędem. Rozwiązując układ równań $ Ae=r $ otrzymamy poprawkę rozwiązania. Tylko czy to ma sens? Układy z macierzą $ A $ już umiemy łatwo rozwiązywać, tylko to wciąż nie będzie idealne, bo znów mamy przybliżenie.
\paragraph{Iteracyjne poprawianie rozwiązań}
$$\begin{matrix}
 x^{0}&=&x \\
 x^{(k+1)}&=&x^{(k)}+A^{-1}r^{(k)}, &k=0,1,\ldots 
\end{matrix}$$


Żeby poprawianie poprawiało, obliczanie wektora residualnego musi być wykonane w jak największej precyzji.
\subsection{Wartości i wektory własne}
$ \lambda, v $ - para własna dla $A$, spełnia $ Av=\lambda v $
$$||Av||=||\lambda v||$$
$$||Av||=|\lambda|||v||$$
$$\frac{||Av||}{||v||}=|\lambda|$$
Czyli dla dowolnej wartości własnej i dowolnej normy indukowanej zachodzi:
$$\sup_{v\not=0}\frac{||Av||}{||v||}\ge|\lambda|\implies |\lambda|\le||A||$$
To się przydaje w metodzie Richardsona.

\paragraph{Twierdzenie Gerszgorina} - o lokalizacji wartości własnych. Każda wartość własna macierzy $ A $ leży co najmniej w jednym z kół na płaszczyźnie zespolonej:
$$ D_i=\{z\in\mathbb{C}:|z-a_{ii}|\le \sum_{j=1, j\not=i}^{n}|a_{ij}|\} \text{ dla }  i=1,2,\ldots, n $$
\begin{proof}
	Weźmy dowolną wartość własną $ \lambda $ z wektorem $ v $. Pokażemy, że istnieje wiersz $ i $ macierzy $ A $ t. że $ \lambda\in D_i $. Niech $ ||v||_\infty=1 $ co implikuje że $ |v_i|=1 $. Teraz $ (Av)_i=\lambda v_i=\sum_{j=1}^n a_{ij}v_j $\\
	
	$$(\lambda-a_{ii}) v_i=\sum_{j=1, j\not=i}^n a_{ij}v_j$$
	$$|(\lambda-a_{ii}) v_i|=|\sum_{j=1, j\not=i}^n a_{ij}v_j|$$
	$$|(\lambda-a_{ii}) v_i|\le\sum_{j=1, j\not=i}^n|a_{ij}||v_j|\le\sum_{j\not=i}|a_{ij}|$$
\end{proof}
\paragraph{Wniosek.} Macierz diagonalnie dominująca nie ma zerowych wartości własnych, czyli jest nieosobliwa.

\section{Ćwiczenia 16/11}
\subsection{Wyznaczanie wektorów i wartości własnych - c.d.}
\paragraph{Metody wyznaczania} - dla $ Av_i=\lambda_iv_i $

\begin{itemize}
	\item Metoda potęgowa - Zaczynamy od wektora $x_0$ i mnożymy z lewej przez $ A $. Zakładamy $|\lambda_1|>|\lambda_2|\ge\ldots\ge|\lambda_n| $ oraz $ A $ ma $ n $ wektorów własnych. Robimy: $$ x_0, \|x_0\|_2=1 $$ 
	$$y_{k+1}=Ax_k, x_{k+1}=\frac{y_{k+1}}{\|y_{k+1}\|_2}$$
	Na koniec wartość własną dostajemy: 
	$$\sigma_k=\frac{x_k^TAx_k}{x_k^tx_k}=x^T_ky_{k+1} $$
	\item Odwrotna metoda potęgowa. 
	$$ x_0, \|x_0\|_2=1 $$ 
	$$(A-\mu I)y_{k+1}=x_k$$
	$$x_{k+1}=\frac{y_{k+1}}{\|y_{k+1}\|_2}$$
	$$\sigma_k=x^T_ky_{k+1} $$
	Jeśli $ \mu=0 $ to $ |\lambda_n| $ musi być ostro mniejsze, bo w normalnej $ |\lambda_0| $ musiało być ostro większe, a $ v_i=\lambda_iA^{-1}v_i\implies A^{-1}v_i=\frac1{\lambda_i}v_i$
	Wartości własne $ (A-\mu I)^{-1} $ to $ \frac{1}{\lambda_i-\mu} $
	\item Co jeśli $|\lambda_1|=|\lambda_2|>|\lambda_3|\ge \dots$? Dostaniemy wektor będący kombinacją liniową wektorów własnych odpowiadających $ \lambda_1 $ i $ \lambda_2 $.
	\item Mamy $|\lambda_1|>|\lambda_2|>|\lambda_3|\ge\ldots\ge|\lambda_n| $. Wyznaczyliśmy $ \lambda_1 $ z metody potęgowej. Jak wyznaczyć $|\lambda_2|$?
	Przyjmujemy $ x_0=\sum_{i=2}^{n}\alpha_iv_i $ jeśli $\{v_i\}  $ baza ortogonalna: wybieramy $ x_0\bot v_1 $. Wtedy co kilka  kroków trzeba $ x_k $ ortogonalizować, żeby zachować ortogonalność utraconą ze względu na błędy zmiennoprzecinkowe.
\end{itemize}
Dane są:
$$A=\begin{bmatrix}
2&1\\
1&2\\
\end{bmatrix}, x_0=\begin{bmatrix}
2\\
1\\
\end{bmatrix}$$ Czy metoda potęgowa dla A jest zbieżna dla $ x_0 $? Policzmy ręcznie i zobaczmy XDD\\
Wartości własne: rozwiążmy $ \det(A-\lambda I) =0$
$$\det(A-\lambda I) = (2-\lambda)(2-\lambda)-1=0$$
$$\lambda_1=3, \lambda_2=1$$
Czyli wektory własne:
$$v_1=\begin{bmatrix}
1\\
1\\
\end{bmatrix}, v_2=\begin{bmatrix}
2\\
1\\
\end{bmatrix}$$
$$v_1=\begin{bmatrix}
1\\
-1+\varepsilon\\
\end{bmatrix}$$
$$Ax_0=\begin{bmatrix}
1+\varepsilon\\
-1+2\varepsilon\\
\end{bmatrix}$$ Z epsilona zrobiły się dwa epsilony, w następnym 4 i 5, pottem 13 i 14. Iloraz współczynników przy epsilonach zbiega do jedynki, więc wynikowy wektor będzie w dobrym kierunku, ale możemy zbiec do wektora do którego mieliśmy dalej.
\subsection{Metoda QR}
$$A_0=A$$
$$A_k=Q_kR_k - \text{rozkład QR}$$
$$A_{k+1}=R_kQ_k - \text{mnożymy na odwrót}$$
Wyznaczanie rozkładów jest kosztowne, ale da się łatwiej używając macierz Hessenberga która jest podobna do macierzy $ A $ (czyli ma te same wartości własne), a metoda QR zachowuje hessenbergowość.
\begin{proof}
	Mamy pokazać, że jeśli macierz Hessenberga $ A =QR$  to macierz $ RQ $ też jest Hessenberga.
	\begin{enumerate}
		\item Jeśli A - Hessenberga to Q też:
		$$\begin{bmatrix}
		x&\ldots&&&&x\\
		x&x&\ldots&&&x\\
		&x&x&\ldots&&x\\
		&&\ddots&&&x\\
		&&&x&x&x\\
		&&&&x&x\\
		\end{bmatrix}=\begin{bmatrix}
			&&&&&\\ \\ \\ \\ \\ \\
		\end{bmatrix}\begin{bmatrix}
		x&\ldots&&&&x\\
		&x&\ldots&&&x\\
		&&x&\ldots&&x\\
		&&&\ddots&&x\\
		&&&&x&x\\
		&&&&&x\\
		\end{bmatrix}$$
		Każda kolejna kolumna Q to kolumna A odpowiednio przemnożona.
		\item Iloczyn $ RQ $ tj, trójkątna górna razy Hessenberga daje macierz Hessenberga. Rozpisać tak samo.
	
	\end{enumerate}
\end{proof} 
\paragraph{Jak sprowadzić macierz do postaci Hessenberga przy pomocy podobieństw?}
Używamy Householdera. Dlaczego by nie do trójkątnej? Pomnożymy z lewej strony i dostaniemy zera w pierwszej kolumnie, ale potem pomnożymy z prawej żeby było podobieństwo i rozwali nam to pierwszą kolumnę. Jeśli zrobimy tak, żeby nie tykać pierwszego wiersza, to z prawej ten sam householder nie zmieni nam pierwszej kolumny. 
Dowód poprawności:
$$A_k=\begin{bmatrix}
	B&F^T\\
	D&E
\end{bmatrix}$$
Gdzie $ B $ jest Hessenberga $ k\times k$, a D ma zera we wszystkich kolumnach poza ostatnią, w której jest wektor $ d $.
Dobieramy $ \tilde{H_k} $ tak aby $ \tilde{H_k}d=\alpha \tilde{e_1} $
Wtedy $ H_k=\begin{bmatrix}
I&0\\
0&\tilde{H_k}
\end{bmatrix} $
Teraz $$
A_{k+1}=H_kA_kH_k=A_k=\begin{bmatrix}
B&F^T\\
\tilde{H_k}D&\tilde{H_k}E
\end{bmatrix}H_k=\begin{bmatrix}
B&F^T\tilde{H_k}\\
\tilde{H_k}D\tilde{H_k}&\tilde{H_k}E\tilde{H_k}
\end{bmatrix}
$$
W ten sposób otrzymujemy $ H_{n-2}\dots H_1AH_i\dots H_{n-2}=T $, $ T $ jest postaci Hessenberga i jest podobna do $ A $.
\section{Ćwiczenia n+2}
\subsection{Arytmetyka $ fl $}
W arytmetyce $ fl $ nie ma łączności w działaniach, np. 
$ 1+\gamma+\gamma, \gamma=\frac32 10^{-16} $
Precyzja arytmetyki to około $ 2.2\cdot 10^{-16} $. Policzmy: 
$$ fl(1+\gamma+\gamma)=fl((1+\gamma)+\gamma) = fl(fl(1+\gamma)+\gamma)$$
$$fl(1+\gamma)=1$$, bo najmniejsza reprezentowalna liczba większa od 1 to $ 1+\varepsilon $. Zatem wynikiem jest 1. Ale co jeśli inaczej znawiasujemy? Mnożenie przez 2 jest dokładne.
$$ fl(1+\gamma+\gamma)=fl(1+(\gamma+\gamma)) = fl(1+fl(\gamma+\gamma)) =fl(1+2\gamma) \not=1\text{, bo } 2\gamma>\varepsilon$$

\subsection{Utrata cyfr znaczących przy odejmowaniu}
$$x=0.3721478693,\ y=0.3720230572,\ x-y=?$$
Mamy system który obsługuje tylko 5 cyfr znaczących.
$$rd(x)=0.37215,\ rd(y)=0.37202$$
W naszym systemie uzyskamy wynik $ fl(x-y)=0.00013 $, podczas gdy w idealnej arytm. $ x-y=0.0001248121 $. Błąd bezwględny nie jest duży, ale względny: $ \frac{fl(x-y)-(x-y)}{x-y}\simeq 4\cdot10^{-2}$. Arytmetyka ma dokładność rzędu $ 10^{-5} $, a nasz błąd jest rzędu aż $ 10^{-2} $! Jest to związane z tym, że liczby które od siebie odejmujemy są bliskie sobie.
\paragraph{Jak policzyć $ a^2-b^2 $?} 
Wersja 1:
\begin{lstlisting}[escapeinside={(*}{*)}]
s := a * a;
t := b * b;
w := s - t;
\end{lstlisting}
Wersja 2: 
\begin{lstlisting}[escapeinside={(*}{*)}]
u := a + b;
v := a - b;
w := u * v;
\end{lstlisting}

Mamy zagwarantowane, że wszystkie działania spełniają 
$$ fl(x\cdot y)=(x\cdot y)(1+\nu), |\nu|\le \varepsilon, \cdot\in\{+,-,*,/\} $$
Ale to zakłada, że liczby są dokładnie reprezentowane!
Jakie są błędy naszych "algorytmów"?

\begin{enumerate}
	\item$fl(a^2-b^2) = [a^2(1+\delta_1)-b^2(1+\delta_2)](1+\delta_3) = a^2(1+\nu_1)-b^2(1+\nu_2), $ gdzie $ \nu_1=\delta_1+\delta_3+\delta_1\delta_3, \nu_2=\delta_2+\delta_3+\delta_2\delta_3 $.
	$ \frac{fl(a^2-b^2)-(a^2-b^2)}{a^2-b^2} = \frac{a^2\nu_1-b^2\nu_2}{a^2-b^2} $
	Błąd względny zależy od danych! Jeśłi $ a^2, b^2 $ są bliskie i duże i dodatkowo błędy $ \nu_1, \nu_2 $ są przeciwnego znaku, to błąd względny jest DUŻY!
	\item $ fl(a^2-b^2)=[(a+b)(1+\delta_1)(a-b)(1+\delta_2)](1+\delta_3)\\ =  (a^2-b^2)(1+\delta_1)(1+\delta_2)(1+\delta_3) = (a^2-b^2)(1+E)\\
	 E=\delta_1+\delta_2+\delta_3 + \delta_1\delta_2+\delta_2\delta_3+\delta_1\delta_3+\delta_1\delta_2\delta_3$
	Poza pierwszymi trzema składnikami, reszta jest grubo poniżej dokładności arytmetyki, zatem $ |E|\tilde{\le} 3\varepsilon $!
\end{enumerate}
Zatem licząc różnicę kwadratów, zawsze liczmy wzorem skróconego mnożenia.
\subsection{Uwarunkowanie zadania}
Wcześniej były dokładne dane i niedokładne obliczenia, teraz mamy dokładne obliczenia na niedokładnych danych. Policzmy uwarunkowanie zadania różnicy kwadratów.\\
Zaburzone dane:
$$\tilde{a}=a(1+\delta_1),\ \tilde{b}=b(1+\delta_2), \ |\delta_1|\le\varepsilon$$
$$|\frac{\tilde{a}^2-\tilde{b}^2-(a^2-b^2)}{a^2-b^2}| = |\frac{a^2(1+\delta_1)^2-b^2(1+\delta_2)^2-(a^2-b^2)}{a^2-b^2}| \tilde{\le}2\frac{a^2+b^2}{|a^2+b^2|} 
\text{ - wskaźnik uwarunkowania} $$ 
Wskaźnik uwarunkowania jest wysoki, co oznacza że niedokładne dane mają duży wpływ na błąd, czyli zadanie jest źle uwarunkowane. Uwaga, tutaj nie miało znaczenia którego algorytmu użyliśmy! Dla niedokładnych danych oba algorytmy dadzą niedokładne wyniki.
\subsection{Numeryczna poprawność algorytmu}
\paragraph{Jak obliczyć $ f(x)=1-\cos x $?}
Dla $ x\approx 0 $ mamy $ \cos x\approx 1 $.
Jak przekształcić? $$ \cos x=\cos2\frac x2=\cos^2\frac x2-\sin^2\frac x2 $$ Wtedy 
$$ 1-\cos x= \cos^2\frac x2+\sin^2\frac x2-\cos^2\frac{x}{2}+\sin^2\frac x2=2\sin^2\frac x2$$
Dzielenie przez 2 jest dość dokładne, a f. tryg. i tak musimy policzyć.
$$g(x)=\sqrt{x^2+1}-x$$
Mamy utratę precyzji gdy $ x>>1 $
$$g(x)=\sqrt{x^2+1}-x=\frac1{\sqrt{x^2+1}+x}$$
\paragraph{Jak policzyć iloczyn skalarny $ x^Ty $?}
$$ x^Ty=\sum_{i=1}^n x_iy_i $$
Sprawdźmy uwarunkowanie:
$$\tilde{x}_i=x_i(1+\delta_i)$$
$$\tilde{y}_i=y_i(1+\gamma_i)$$
$$|\frac{\sum\tilde{x}_i\tilde{y}_i-\sum x_iy_i}{\sum x_iy_i}| \approx |\frac{\sum\tilde{x}_i\tilde{y}_i(\delta_i+\gamma_i)-\sum x_iy_i}{\sum x_iy_i}|\le2\frac{\sum|x_i||y_i|}{|\sum x_iy_i|}\varepsilon$$
Zadanie jest niestety źle uwarunkowane.

\section{Ćwiczenia 30/11}
\paragraph{Znajdź uwarunkowanie zadania obilczania $ Ax $ ze względu na zaburzenia macierzy $ A $}
Zaburzenie macierzy $ \Delta $ - macierz\\
mamy $ \tilde{A}=A+\Delta $\\
Zaburzenie względne:
$$\frac{\|\tilde{A}-A\|}{\|A\|}=\frac{\|\Delta\|}{\|A\|}$$
uwarunkowanie zadania:
$$
	\frac{\|\tilde{A}x-Ax\|}{\|Ax\|} =
	\frac{\|\Delta x\|}{\|Ax\|} \le 
	\frac{\|\Delta\|\| x\|}{\|Ax\|}\frac{\|A\|}{\|A\|} =
	\frac{\|A\|\| x\|}{\|Ax\|}\frac{\|\Delta\|}{\|A\|}
$$
Współczynnik $ \frac{\|A\|\| x\|}{\|Ax\|} $ przy zaburzeniu względnym macierzy jest większy lub równy 1.
\paragraph{Eliminacja Gaussa}
$$A=\begin{bmatrix}
\varepsilon&1\\
1&1
\end{bmatrix}, b= \begin{bmatrix}
1\\2
\end{bmatrix}, \varepsilon\in(0,1)$$
Wtedy $ x=\frac{1}{1-\varepsilon} , y=1-\frac{\varepsilon}{1-\varepsilon}$.\\
Eliminacja Gaussa daje:
$$A=LU, L=\begin{bmatrix}
1&0\\
\frac1\varepsilon&1
\end{bmatrix}, U=\begin{bmatrix}
\varepsilon&1\\
0&1-\frac1\varepsilon
\end{bmatrix}
$$
Co jeśli $ \varepsilon $ jest bliski 0?
$$ fl(U) = \begin{bmatrix}
\varepsilon&1\\
0&-\frac1\varepsilon
\end{bmatrix} $$
$$Lz=\begin{bmatrix}1\\2\end{bmatrix},
z=\begin{bmatrix}z_1\\z_2\end{bmatrix}, z_1=1, z_2=2-\frac1\varepsilon$$
Ale $ fl(z_2)=-\frac1\varepsilon $!
$$fl(U)\begin{bmatrix}
x\\y
\end{bmatrix}=\begin{bmatrix}
1\\-\frac{1}{\varepsilon}
\end{bmatrix}=fl(z)$$
Wyszło $ x = 0, y = 1 $!
\paragraph{Z wyborem elementu głównego!}
po przestawieniu:
$$\begin{bmatrix}1&1\\\varepsilon&1\end{bmatrix}
\begin{bmatrix}x&y\end{bmatrix} = 
\begin{bmatrix}2&1\end{bmatrix}$$
$$L=\begin{bmatrix}
1&0\\
\varepsilon&1
\end{bmatrix}, U=\begin{bmatrix}
1&1\\
0&1-\varepsilon
\end{bmatrix}
$$
$$
fl(U)=\begin{bmatrix}
1&1\\
0&1
\end{bmatrix}
$$
Dla małych $ \varepsilon $ wynik wychodzi $ x=1, y=1 $ - znacznie lepiej! Wybór elementu głównego daje nam numeryczną poprawność.
\paragraph{Uwarunkowanie zadania}
$$ cond(A)=\|A\|\|A^{-1}\| $$
$$ \|A\|_1=2 $$
$$ \|A^{-1}\|_1= $$
$$ A^{-1}= \frac{1}{\varepsilon-1}\begin{bmatrix}
1&-1\\
-1&\varepsilon
\end{bmatrix}$$
$$ \|A^{-1}\|_1= \frac2{1-\varepsilon}$$
Dla małego $ \varepsilon $ macierz jest dobrze uwarunkowana, jednak bez wyboru el. gł. wynik jest słaby, więc algorytm nie jest numerycznie poprawny.
\subsection{UwUarunkowanie LZNK}
$$A=\begin{bmatrix}
1&1\\
\varepsilon&0\\
0&\varepsilon
\end{bmatrix}$$
\paragraph{Układem równań normalnych}
$$A^TAx=A^Tb$$
$$A^TA=\begin{bmatrix}
1+\varepsilon^2&1\\
1&1+\varepsilon^2
\end{bmatrix}$$
Ale float!
$$fl(A^TA)=\begin{bmatrix}
1&1\\
1&1
\end{bmatrix}$$
Trzeba używać Householderae albo Givensa.
\subsection{Interpolacja}
Dane: $ f:\mathbb{R} \rightarrow \mathbb{R} $\\
Szukamy wielomianu, $ w(x) $ stopnia $n$ t. że $ w(x_i)=f(x_i) $ dla $ i=0,1,\dots,n $.
\paragraph{Bazy wielomianów:}
\begin{itemize}
	\item potęgowa (standardowa) - $ \{1,x,x^2,\dots,x^n\} $
	\item Newtona - $ \{1, x-x_0, (x-x_0)(x-x_1), \dots,  \prod_{i=0}^{n-1}(x-x_i)\} $
	\item Langranża - $ \{l_i\} : l_i(x)=\prod_{j=0, j\not=i}^{n-1}\frac{x-x_j}{x_i-x_j} $
\end{itemize}
\paragraph{Wartości wielomianów w bazie standardowej}
Schemat Hornera!
$$w(x)=\sum_{i=0}^{n}a_ix^i=(a_nx^{n-1}+\dots+a_1)x+a_0$$
Można wyłączać kolejne $ x $ aż do $ (\dots(a_nx+a_{n-1})x + \dots + a_1)x + a_0 $
\begin{lstlisting}[escapeinside={(*}{*)}]
	w = a[n];
	for k = n - 1 downto 0 do
		w = w*x+a[k]
	done
\end{lstlisting}
\#taniejniema - $ n $ dodawań, $ n $ mnożeń\\
\paragraph{Co dla bazy Newtona?}
Też schemat Hornera, tylko zamiast wyciągać $ x $ wyciągamy $ x-x_i $  - $ 2n $ dodawań, $ n $ mnożeń

\paragraph{Co dla bazy Lagaraża?}
	Nie jest to dobry pomysł żeby jej używać w obliczeniach numerycznych.
\paragraph{Czy Lagrażyna to wgl baza?}
$$l_i(x_k)=\bigg\{\begin{matrix}
0&i\not=k\\
1 &\text{wpp}
\end{matrix}$$
\paragraph{To co wgl robimy z Lagryzoniem?}
Postać barycentryczna wielomianu interpolacyjnego Lagrożnego
$$p(x)=\prod_{i=0}^{n}(x-x_i)$$
Nie jest on w bazie Newtona!!!1!
$$l_i(x)=\frac{p(x)}{x-x_i}\prod_{j=0, j\not=i}^n\frac1{x_i-x_j}$$
Niech $ L(x) $ - wielomian interpolujący funkcję $f(x)$.
$$L(x)=\sum_{i=0}^{n}c_iL_i(x)$$
Dla $ x_k $ z całej sumy zostaje tylko $ c_kL_k(x_k)=c_k $.\\
Łatwo wyznaczyć interpolację, trudno nią interpolować!
$$L(x)=
\sum_{i=0}^{n}\frac{p(x)}{x-x_i}\prod_{j=0,j\not=i}^n\frac1{x_i-x_j}f(x_i) = p(x)\sum_{i=0}^{n}\frac{w_i}{x-x_i}f(x_i)
\text{gdzie}
w_i=\prod_{j\not=i}\frac1{x_i-x_j}$$
Pierwsza postać barycentryczna!
Niech $\tilde{L}$ - wielomian interpolujący funkcję $ f(x)\equiv1 $. Jasne jest, że $\tilde{L}\equiv1$.
$$\tilde{L}(x)=p(x)\sum_{i=0}^{n}\frac{w_i}{x-x_i}$$
$$L(x)=\frac{L(x)}{\tilde{L}(x)}=\frac{\sum_{i=0}^{n}\frac{w_i}{x-x_i}f(x_i)}{\sum_{i=0}^{n}\frac{w_i}{x-x_i}}$$
Druga postać barycentryczna - pozbyliśmy się $ p(x) $, co jest dobre question mark?
\section{Ćwiczenia 7/12}
\subsection{Ko-loss}
\subsection{Interpollacja}
Mamy wielomian w bazie newtona:
$$w(x)=\sum_{i=0}^nb_ip_i(x)\ \ \  p_i(x)=\bigg\{\begin{matrix}
1&i=0\\\prod_{j=0}^{i-1}(x-x_i)&i>0
\end{matrix}$$
$$w(x_i)=f(x_i) i=0,\dots,n$$
A $ f(x_i) $ mamy dane!
$$b_i=f[x_0,x_1,\dots,x_i]$$
$$f[x_0,\dots,x_i]=
\frac{
	f[x_1,\dots,x_i]-f[x_0,\dots,x_{i-1}]
}{
	x_i-x_0
}$$
oraz
$$f[x_i]=f(x_i)$$
\paragraph{Ręczna robótka} - mamy dane węzły $ \{2,4,0\} $, wartości w węzłach $ \{11,63,7\} $. Liczymy od najmniejszych przypadków. 
$$
\begin{matrix}
2&11&&\\
4&63&26&\\
0&7&14&6\\
&f[x_j]&f[x_j,x_j+1]&f[x_0,x_1,x_2]
\end{matrix}
$$
Interesują nas tylko wartości na przekątnej, więc można to robić w miejscu. Używamy tych współczynników, które zaczynają się od indeksu zerowego. Wtedy $ w(x)=11+26(x-2)+6(x-2)(x-4) $
\subsection{Błąd interpolacji}
$$f(x)-w(x)=f[x_0,x_1,\dots,x_n,x]p(x), \text{ dla } x\not=x_i$$
Mało przydatne, ale okazuje się że:
$$f[x_1,x_{i+1},\dots,x_{i+k}]=\frac{f^{(k)}(\xi)}{k!}\text{ o ile } f\in C^k, \xi\in[x_i,x_{i+k}]$$
Jak więc zmierzyć ten błąd? Normą supremum. 
$$\|g\|_\infty=\sup_{x\in[a,b]}g(x)$$
U nas:
$$\sup_{x\in[x_0,x_n]}|f(x)-w(x)| = \sup|\frac{f^{(n+1)}(\xi)}{(n+1)!}p(x)| \le \sup|\frac{f^{(n+1)}(\xi)}{(n+1)!}|\sup|p(x)|$$
Zatem: $$\|f-w\|_{\infty, [x_0,x_n]} \le \frac{\|f^{(n+1)}\|_{\infty,[x_0,x_n]}}{(n+1)!} \|p\|_{\infty,[x_0,x_n]}$$
Wybór węzłów mocno wpływa na błąd!
\paragraph{Węzły równo odległe}
$$x_i=x_0+ih,h=\frac{x_n-x_0}{n},i=0,1,\dots,n$$
Wtedy mamy:
$$\|p\|_{\infty,[x_0,x_n]}\le \frac{n!h^{n+1}}{4}$$
\begin{proof}
	dla $n = 1$:\\
	$$ p(x)=(x-x_0)(x-x_1),h=x_1-x_0 $$
	A skoro $ p $ jest stopnia 2 i nieujemne, to najwyższy punkt między miejscami zerowymi ($ x_0, x_1 $) jest dokładnie w połowie, czyli:
	$$\sup_{x\in[x_0,x_1]}|(x-x_0)(x-x_1)|=\frac{h^2}{4}$$
	\paragraph{Krok indukcyjny:} $$\sup_{x\in[x_0,x_{n}]}|(x-x_0)\dots(x-x_n)| = \max\{
		\sup_{x\in[x_0,x_{n-1}]}|\prod_{i}(x-x_i)|,
		\sup_{x\in[x_1,x_{n}]}|\prod_{i}(x-x_i)|
	\}$$
	To pierwsze szacujemy przez: 
	$$ \sup_{x\in[x_0,x_{n-1}]}|\prod_{i}^{n-1}(x-x_i)| \sup_{x\in[x_0,x_{n-1}]}|(x-x_n)|\le \frac{(n-1)!h^n}{4}nh$$
\end{proof}
\subsection{Oszacuj błąd interpolacji} Lagrenżola o normie supremeum dla $ f(x)=\sin x $ na $ [-\frac\pi2;\frac\pi2] $ na węzłach $ \{-\frac\pi2,0,\frac\pi2\} $.\\
Dla $n=2$:
Trzecia pochodna to minus cosinus, jej norma supremum to 1, a norma $ p $ jest mniejsza od $ \frac12(\frac\pi2)^3 $
\section{Ćwiczenia 14/12}
\subsection{Trochę Largo al żatotum}
\paragraph{Błąd interpolacji} $\|f-w\|_\infty \le \frac{\|f^{(n+1)}\|_\infty}{(n+1)!}\|p\|_\infty, p(x)=\prod_{i=0}^n(x-x_i)$
Miejsca zerowe wielomianu czebyszewa stopnia $n$:
$$T_n(x)=\cos(n \arccos x), x\in[-1,1]$$
\paragraph{Twierdzenie. }Wielomiany $ \frac1{2^{n-1}}T_n(x), n \ge 1 $ mają najmniejszą normę $ \|\cdot\|_\infty $ na $ [-1,1] $ spośród wszystkich wielomianów z "1" przy $x^n$
\paragraph{Miejsca zerowe wielomianu $ T_n(x) $ przeskalowane na przedział $ [a,b] $}
Węzły czebyszewa:
$$x_i=\frac{a+b}{2}+\frac{a-b}{2}\cos(\frac{2i+1}{2n+2}\pi), i=0,1,\dots,n$$
$$\|p\|_\infty = (\frac{b-a}{2})^{n+1}\frac{1}{2^n}\text{ dla węzłów czebyszewa}$$
\subsubsection{Zadanie}
Znajdź najmniejsze $ n $ t. że błąd aproksymacji jednostajnej funkcji $ f(x)=\sin x $ na $ [0;4] $ wielomianem interpolacyjnym stopnia $ n $ jest mniejszy niż $ 2^{-10} $ dla węzłów Czebyszewa.
\begin{proof}[Rozwiązanie]
	Czyli minimalizujemy błąd interpolacji $ \|f-w\|_{\infty, [a,b]}<2^{-10} $ - trzeba znaleźć $ n $ takie, że to zachodzi. Wystarczy, że $ \frac{\|f^{(n+1)}\|_{\infty, [a,b]}}{(n+1)!}\|p\|_\infty <2^{-10} $. A dowolna pochodna sinusa spełnia $ \|f^{(n+1)}\|_{\infty, [0,4]}=1 $, bo norma nieskończoność funkcji na przedziale to jej supremum z zbioru wartości bezwględnych wartości funkcji. A $ \|p\|_\infty=(\frac42)^{n+1}\frac{1}{2^n}=2 $, zatem $ \frac{1}{(n+1)!}2<2^{-10}  \implies (n+1)!>2^{11} $, zatem $ n=6 $.
\end{proof}
\subsubsection{Uwaga}
nieważne jak wybierzemy węzły, zawsze się znajdzie funkcja która jest źle przybliżana przez Logrożka. Z drugiej strony, dla każdej funkcji istnieje ciąg wyborów węzłów, dla którego Logaryż jest zbieżny.
\subsection{Interpolacja wielomianowa Hermite'a}
węzły $ \{x_0, \dots, x_k\} $\\
$ w(x) $ to wielomian stopnia $ n $ interpolujący $ f(x) $ t. że 
$$ w^{(j)}(x_i)=f^{(j)}(x_i)\ \ j=0,1,\dots,m_i-1\ \ i=0,1,\dots, k $$
Liczba warunków: $ \sum_{i=0}^km_i=n+1 $\\
Węzły wielokrotne: $ \{x_0, \dots, x_0, x_1, \dots, x_1, \dots, x_k, \dots, x_k\} $ - $ i $-ty węzeł jest wypisany $ m_i $ razy.\\
Baza Newtona: $\{1, (x-x_0), \dots, (x-x_0)^m_0, (x-x_0)^m_0(x-x_1), \dots, \prod_{i}^{k}(x-x_i)^{m_i - [i=k]}\}$
$$f[x_i, x_{i+1}, \dots, x_{i+j}]=\frac{f[x_{i+1},\dots, x_{i+j}]-f[x_i, \dots, x_{i+j-1}]}{x_{i+j}-x_i}$$
Przy czym:
$$\text{Jeśli węzeł ma krotność $ j+1 $ to: }f[x_i, \dots, x_i]=\frac{f^{(j)}(x_i)}{j!}$$
\subsubsection{Zadanie}
Znajdź wielomian interpolacyjny Hermite'a t. że $ w(1)=2, w'(1)=3, w(2)=6, w'(2)7, w''(2)=8 $.
\begin{proof}
	Szukamy wielomianu stopnia 4.
	węzły $ \{1, 1, 2, 2, 2\} $
$$	\begin{array}{c|ccccc}
1&2&&&&\\
1&2&3&&&\\
2&6&4&1&&\\
2&6&7&3&2&\\
2&6&7&4&1&-1
	\end{array}$$
Baza Newtona:
	$$\{1, x-1, (x-1)^2, (x-1)^2(x-2), (x-1)^2(x-2)^2\}$$
	Wielomian:
	$$w(x)=2+3(x-1)+(x-1)^2+2(x-1)^2(x-2)-(x-1)^2(x-2)^2$$
\end{proof}
\subsection{Zadanie}
Udowodnij, że jeśli wśród $ n $ węzłów z $[-1,1]$ znajduje się 0, to wielomian $w$ stopnia $ n-1 $ interpolacyjny Lagorża dla funkcji $ f(x)=\sinh(x)=\frac{e^x-e^{-x}}{2} $ spełnia:  $$ |w(x)-f(x)| \le \frac{2^n}{n!}|f(x)|\ (|x| \le 1)$$
\begin{proof}
$$|w(x)-f(x)|=\frac{|f^{(n)}(\xi)|}{n!}\prod_{i=0}^{n-1}|x-x_i|$$
$$(\sinh(x))'=\cosh(x)=\frac{e^x++e^{-x}}{2}$$
$$|\sinh(x)| \le |\cosh(x)| \le \frac{e+\frac1e}{2} \le 2$$
Jeden z węzłow jest równy 0, bez straty ogólności $ x_0=0 $
$$\prod_{i=0}^{n-1}|x-x_i|=|x|\prod_{i=1}^{n-1}|x-x_i| \le |x|2^{n-1}$$
Okazuje się że $ |x| \le |\sinh(x)| $. Zatem:
$$|w(x)-f(x)|=\frac{|f^{(n)}(\xi)|}{n!}\prod_{i=0}^{n-1}|x-x_i| \le \frac{2}{n!}|x|2^{n-1} \le \frac{2^n}{n!}|\sinh(x)|$$
\end{proof}
\subsection{Splajny}
Dzielimy $ [x_0,x_n] $ na $ n $ podprzedziałów, i na każdym z nich osobno splajn jest wielomianem.
\subsubsection{Splajan liniowy (stopnia 1)}
Jeśli $ s_i(x) $ jest wielomianem stopnia 1, to splajnem liniowym nazywamy funkcję ciągłą $ s $ spełniającą
$$s(x)=s_i(x)\text{ dla }x\in[x_i, x_{i+1}), i=0,1,\dots,n-1$$
Splajny liniowe na $ [x_0, x_n] $ tworzą $ n+1 $-wymiarową przestrzeń liniową.
\subsubsection{Błąd interpolacji dla $ f\in C^2 $}

$$\|f-s\|_{\infty,[a,b]} \le \max_{i=0,\dots,n-1}\|f-s_i\|_{\infty,[x_i, x_{i+1}]}
$$
$$\|f-s_i\|_{\infty,[x_i, x_{i+1}]} \le \frac{\|f''\|}{2}\frac{h_i^2}{4}$$

\section{Ćwiczenia 21/12}
\subsection{Gładkość splajnów liniowych}
Jeśli $ s $ - splajn liniowy interpolujący funkcję $ f\in C^1 $ w węzłach $ x_0<\dots< x_n $, to zachodzi:
$$\int_{x_0}^{x_n}(s'(x))^2dx\le \int_{x_0}^{x_n}(f'(x))^2dx$$
Pochodną liczymy przedziałami i zakładamy że na krawędziach nie mamy wartości, a to nie psuje całkowalności.
\begin{proof}
	Niech $ g=f-s $.
$$\int_{x_0}^{x_n}(f'(x))^2dx=\int_{x_0}^{x_n}(f'(x)-s'(x)+s'(x))^2dx=$$
$$\int_{x_0}^{x_n}(g'(x))^2dx+\int_{x_0}^{x_n}(s'(x))^2dx+2\int_{x_0}^{x_n}g'(x)s'(x)dx$$
Zauważmy, że $s'$ jest przedziałami stała:
$$\int_{x_0}^{x_n}g'(x)s'(x)dx=\sum_{i=0}^{n-1}\int_{x_i}^{x_{i+1}}g'(x)s'(x)dx=\sum_{i=0}^{n-1}c_i\int_{x_i}^{x_{i+1}}g'(x)dx=\sum_{i=0}^{n-1}c_i(g(x_{i+1}) - g(x_i))=0$$
Zatem:
$$\int_{x_0}^{x_n}(g'(x))^2dx+\int_{x_0}^{x_n}(s'(x))^2dx+2\int_{x_0}^{x_n}g'(x)s'(x)dx=$$
$$\int_{x_0}^{x_n}(g'(x))^2dx+\int_{x_0}^{x_n}(s'(x))^2dx$$
\end{proof}
\subsection{Splajny kubiczne}
Niech $ s $ - splajn kubiczny na węzłach $ x_0 < \dots< x_n $
$$s(x)=s_i(x), x\in [x_i, x_{i+1})$$
$$s_i(x) \text{ jest wielomianem 3 stopnia na } [x_i, x_{i+1}]$$
$$s\in C^2([x_0,x_n])$$
$$s_i^{(j)}(x_{i+1})=s_{i+1}^{(j)}(x_{i+1}) \text{ dla } j=0,1,2$$
Interpolujemy: 
$ s(x_i)=f(x_i), i=0,\dots,n $
\subsubsection{Zadanie}
Niech $ s $ - splajn kubiczny t. że na $ [x_{k-1}, x_k] $ i $ [x_{k+1}, x_{k+2}] $ $ s \equiv0$. Czy pomiędzy też musi?
\begin{proof}[Rozwiązanie]
$$s_k(x)=a+b(x-x_k)+c(x-x_k)^2+d(x-x_k)^3$$
Po pierwsze $ s_k(x_k)=0\implies a=0 $.\\
Po drugie, $ s_k'(x_k)=b+2c(x-x_k)+3d(x-x_k)^2=0\implies b=0 $. \\
Po trzecie, $ s_k''(x_k)=2c+6d(x-x_k)=0\implies c=0 $ \\
Więc wiemy, że $ s_k(x)= d(x-x_k)^3$.\\
A skoro musi być, że $ s_k(x_{k+1})=0$, oraz $ x_k\not=x_{k+1} $, to $ d(x_{k+1}-x_k)^3=0\implies d=0 $.\\
A więc musi.
\end{proof}
Zadanie można też rozwiązać z interpolacji Pustelnika na dwóch podwójnych węzłach: $ \{x_k, x_k, x_{k+1}, x_{k+1}\} $. Konstrukcja tabelki pokazuje, że wszędzie są zera, więc i funkcja też musi być zerem.
\subsection{Gładkość splajnów kubicznych}
Jeśli $ f\in C^2, a=x_0<\dots<x_n=b $, $ s $ - splajn kubiczny naturalny (czyli drugie pochodne na końcach są zerem), to wielomian interpolujący $ f $ w węzłach $ x_0, \dots, x_n $, to:
$$\int_{a}^{b}(s''(x))^2dx\le\int_{a}^{b}(f''(x))^2dx$$
\begin{proof}
Dowód ten sam co wyżej tylko do pokazania że całka iloczynu jest zerem trzeba użyć całkowania przez części i rozpisać sumę, a tam się wszystko zje poza pierwszym i ostatnim kawałkiem, które się zerują bo druga pochodna na końcach jest zero.
\end{proof}
\subsection{B-splajny}
$$B_i^0=\bigg\{\begin{matrix}
1&x_i\le x<x_{i+1}\\
0&\text{wpp}
\end{matrix}$$
$$B^k_i(x)=\frac{x-x_i}{x_{i+1}-x_i}B_i^{k-1}(x)+\frac{x_{i+k+1}-x}{x_{i+k+1}-x_{i+1}}B_{i+1}^{k-1}(x)$$
Jeśli $ s $- splajn kubiczny, to $ s $ można zapisać w tej bazie:
$$s(x)=\sum_{j=-3}^{n-1} c_jB_j(x), x\in[x_0,x_n]$$
\subsubsection{Jak wyznaczyć bazę dla splajnu interpolacyjnego naturalnego?}
$$\sum_{j=-3}^{n-1}c_jB_j(x_i)=c_{i-3}(x_i)+c_{i-2}B_{i-2}(x_i) + c_{i-1}B_{i-1}(x_i)=f(x_i)$$
A to jest równanie liniowe, z macierzą 5-diagonalną, (ale na zewnętrznych diagonalach jest tylko po jednej jedynce, więc gg ez)

\section{Ćwiczenia styczeń}
\subsection{Aproksymacja}
$X$ - przestrzeń unormowana
$V\subset X$ - podprzestrzeń, $ \dim V=n $
$$f\in X$$
Szukamy $h^*\in V$ t. że $ \|h^*-f\|\le \|f-g\|\forall_{g\in V} $
\subsection{Aproksymacja średniokwadratowa}
$ X $ - przestrzeń Hilberta, iloczyn sklarany $(\cdot, \cdot)$, $ \|f\|=\sqrt{(f, f)} $ (norma druga)
Element najlepszej aproksymacji (ENA) $ h^* $ - $ (f-h^*,g)=0 \forall_{g\in V} $, czyli ortogonalny do $ V $.
$ \{g_1, \dots, g_n\} $ - baza $ V $
$ h^*=\sum_j c_j g_j $
$$(f-\sum_j c_j g_j, g_i)=0$$
$$\sum_j c_j(g_j, g_i)=(f, g_i), i=1,\dots,n$$
zatem macierz układu:
$$Gc=F, F=[(f,g_i)]i=1\dots,n$$
$ G $ to macierz grama $ G=[(g_j,g_i)]i,j=1\dots,n $
Macierz grama jest zwykle źle uwarunkowama. Ale co jak nasza baza będzie ortogonalna ($ \{\varphi_1, \dots, \varphi_n\} $)? iloczyny skalarne poza przekątną są zerowe :3
co nam daje $ c_i(\varphi_i, \varphi_i)=(f, \varphi_i) $, czyli $ h^*=\sum_{j=1}^n \frac{(f,\varphi_j)}{(\varphi_j, \varphi_j)}\varphi_j$

Błąd aproksymacji: $ \|f-h^*\| $
dla bazy ortogonalnej:
$\|f-h^*\|^2=\|f\|^2-\|h^*\|^2=\|f\|^2-\sum_{j=1}^n \frac{(f,\varphi_j)^2}{(\varphi_j, \varphi_j)}$
\subsection{"Formuła dwuczłonowa" dla wielomianów ortogonalnych względem iloczynu skalarnego $ (\cdot, \cdot) $}
Ciągiem $ a $ możemy przeskalować bazę względem bazy standardowej.\\
$\varphi_{-1}\equiv0,\varphi_0=a_0$
$$\varphi_k(x)=(\alpha_kx-\beta_k)\varphi_{k-1}(x)-\gamma_k\varphi_{k-2}(x)\ \  k=1,2,\dots$$
$$\text{gdzie } \alpha_k=\frac{a_k}{a_{k-1}}, \beta_k=\alpha_k\frac{(x\varphi_{k-1},\varphi_{k-1})}{(\varphi_{k-1},\varphi_{k-1})}, \gamma_k=\frac{\alpha_k}{\alpha_{k-1}}\frac{(\varphi_{k-1},\varphi_{k-1})}{(\varphi_{k-2},\varphi_{k-2})}$$
\subsection{Zadanie}
Znajdź ENA w przestrzeni wielomianów st. 1 dla $f(x)=e^x; (f,g)=\int_{0}^{1}f(x)g(x)\d d x$
Baza: $\{1, x\}$, macierz Grama:
$$
\begin{Bmatrix}
\int_{0}^1 1\cdot 1\d d x & \int_{0}^1 x\d d x\\
\int_{0}^1 x\d d x&\int_{0}^1 x^2\d d x
\end{Bmatrix}
=
\begin{Bmatrix}
1 & \frac12\\
\frac12&\frac13
\end{Bmatrix}
$$
Wychodzi macierz Hilberta, co pokazuje że warto znajdować bazę ortogonalną, o ile używamy formuły trójczłonowej.
Można też ortogonalizować Gramem Schmidtem, ale on jest numerycznie niepoprawny, chyba że zastosuje się go 2 razy, ale wtedy duży koszt.
$$ (f, g_0)=\int_{0}^1e^x\d dx=e-1 $$
$$ (f, g_0)=\int_0^1xe^x=xe^x|^1_0-\int_{0}^1e^x\d dx=e-(e-1)=1$$
Teraz to samo z ortogonalizacją:
Ujednoznacznienie: przyjmujemy $ a_0=1,\alpha_k=1,\ \ k=1,2,\dots $
$$\varphi_0(x)=1$$
$$\varphi_1(x)=(x-\beta_1)\varphi_0(x)$$
$$\beta_1=\frac{(x\varphi_{0},\varphi_{0})}{(\varphi_{0},\varphi_{0})}=\frac12$$
$$\varphi_1(x)=x-\frac{1}{2}$$
$$h^*=\frac{(f,\varphi_0)}{(\varphi_0,\varphi_0)}\varphi_0+\frac{(f,\varphi_1)}{(\varphi_1,\varphi_1)}\varphi_1 $$
$$(f,\varphi_1)=\int_{0}^1(x-\frac12)e^x\d dx=\int_{0}^1xe^x\d dx-\frac12\int_{0}^1e^x\d dx=1-\frac12(e-1)=\frac{3-e}{2}$$
$$(\varphi_1,\varphi_1)=\int_{0}^{1}(x-\frac12)^2\d dx=\frac{(x-\frac12)^3}{3}|^1_0=\frac12$$
Zatem:
$$h^*(x)=e-1 + 6(3-e)(x-\frac12)$$
\subsection{Zadanie 2 electric boogaloo}
Znaleźć ENA w przestrzeni wielomianów stopnia 1 przy iloczynie skalarnym dyskretnym: $ (f,g)=\sum_{i=1}^{4}f(x_i)g(x_i) $, gdzie $ x_0=-1, x_1=0, x_2=1, x_3=2 $ dla funkcji $ f: $
$$\begin{array}{c|c|c|c|c|}
x_i&-1&0&1&2\\
\hline
f(x_i)&5&-3&-1&1
\end{array}$$
baza $ \{g_0,g_1\}=\{1,x\} $
$$(g_0,g_0)=\sum 1\cdot 1=4$$
$$(g_0,g_1)=-1+0+1+2=2$$
$$(g_0,g_0)=1+0+1+2=6$$
Zatem:
$$
G=\begin{bmatrix}
4&2\\
2&6
\end{bmatrix}
F=\begin{bmatrix}
2\\
-4
\end{bmatrix}
$$
Zróbmy dla tej funkcji LZNK
$ w(x)=ax+b $
$$A=\begin{bmatrix}
1&-1\\1&0\\1&1\\1&2\\
\end{bmatrix}
b=\begin{bmatrix}
5\\-3\\-1\\1
\end{bmatrix}
$$
To wtedy układ równań normalnych $ A^TAz=A^Tb $ da nam układ równań z $ G $ i $ F $ tymi co wyżej.
\subsection{Aproksymacja jednostajna}
Używamy normy supremum.
$$\|f\|=\sup_x |f(x)|$$
W ogółności nie ma jednonacznego rozw.
\subsubsection{Przykład}
znaleźć funkcję $ g\in U=span\{x\} $ najlepiej aproksymującą $ f(x)=\cos x$ na $ [0,\frac\pi2] $
$\|f-g^*\|\le \|f-g\|\forall_{g\in U}$
$g_\lambda(x)=\lambda x$
dla $\lambda\in[0,\frac2\pi], g_\lambda$ jest ENA.\\
Ale dla przestrzeni wielomianów rozwiązania są jednoznaczne.
\subsubsection{Zadanie}
Wyznacz ENA w przedziale wielomianów st. 1 dla $ f(x)=\sqrt x na [0,1]  $ w sensie aproks. jednost.
Czyli szukamy funkcji liniowej, której największe oddalenie od pierwiastka na przedziale $ [0;1] $ jest jak najmniejsze.
$$w(0)-f(0)=\delta$$
$$w(x^*)-f(x^*)=-\delta$$
$$w(1)-f(1)=\delta$$
$$w'(x^*)-f'(x^*)=0$$
Zatem:
$$a=\delta$$
$$\delta+bx^*-\sqrt{x^*}=-\delta$$
$$\delta+b-1=\delta\implies b=1$$
$$b=\frac{1}{2\sqrt{x^*}}$$
$$x^*=\frac14$$
$$\delta=\frac18$$
$$w(x)=x+\frac18$$
\section{Ćwiczenia dalej}
\subsection{Zadanso}
Funkcja $ f $ ciągła na przedziale $ I $: $ f\in C(I) $. Szukamy funkcji stałej najlepiej aproksymującej $ f $ w normie $ \sup $.
Rozwiązanie: znajdujemy ekstrema i bierzemy średnią.
\paragraph{Twierdzenie o alternansie Czebyszewa}
Niech $ w(x) $ - wielomian stopnia $ n $.\\
Alternans: układ punktów $ \{x_i\}, i=0,\dots, n+1$ taki że: $$ f(x_i) - w(x_i)=s(-1)^i\|f-w\|_\infty, s=\pm 1 $$
Wtedy $ w $ jest ENA w normie $ \sup \iff$ istnieje alternans.
\subsection{Zadanso 2}
Niech $ f(x)=x^{n+1} $. Szukamy wileomianu stopnia $ n $ najlepiej aroksymującego $ f $ nna $ [-1,1] $ w normie $\sup$.
Czyli minimalizujemy $ \|f-w\|_\infty $.
$ f-w  $ jest wielomianem stopnia $ n+1 $. Z twierdzenia o węzłach czebyszewa (one minimalizują normę supremum):\\
$ T_{n+1}(x) $ - wielomian Czebyszewa\\
$ \tilde{T}_{n+1}(x)=\frac{1}{2^n}T_{n+1}(x) \leftarrow$  ma 1 przy $ x^{n+1} $ oraz najmniejszą normę sup wśród wielomianów stopnia $ n + 1 $.
Zatem $ f-w=\tilde{T}_{n+1}\implies w(x)=x^{n+1}-\tilde{T}_{n+1}(x) $
\subsection{Algorytm Clenshawa}
Obliczanie wartości wielomianu: 
$$ w(x)=\sum_{i=0}^{n}c_i\varphi_i(x), \{\varphi_i\}\text{  - układ ortogonalny} $$
Potrzeba współczynników z formuły trójczłonowej.
\begin{lstlisting}[escapeinside={(*}{*)}]
d[n+2]=d[n+1]=0;
for k = n, n-1, (*$\dots$*), 0:
	d[k]= ((*$\alpha$*)[k+1] * x - (*$\beta$*)[k+1]) * d[k+1] - (*$\gamma$*)[k+2] * d[k+2] + c[k]
w(x) := d[0] * a[0]
\end{lstlisting}
Wyprowadza się to rozpisując $ \varphi_n(x) $ z formuły trójczłonowej
\subsection{Równania nieliniowe}
$ x^*:f(x^*)=0 $
\subsubsection{Metoda bisekcji}
Startujemy z przedziału $ [a,b] $ t. że $ f(a)f(b)<0 $ dla $ f\in C([a,b]) $
Bierzemy $ c=\frac{a+b}{2} $ i do następnego kroku bierzemy ten z przedziałów $ [a,c], [c,b] $ dla którego $ f $ ma na końcach różne znaki.
\paragraph{Błąd} -  W $ n+1 $-szym kroku $ e_{n+1}=\frac12 e_n $.
Zbieżna globalnie.
\subsubsection{Metoda Newtona (stycznych)}
$ x_0 $ - dane\\
$ x_{n+1} =x_n-\frac{f(x_n)}{f'(x_n)},\ \ n=0,1,\dots$
czyli znajdujemy punkt przecięcia stycznej w $ x_n $ z osią $ x $. 
Jakie założenia chcemy? $ f\in C^1 $ na pewno, jak chcemy kwadratową zbieżność to potrzeba $ f\in C^2 $. Do tego trzeba też $ f'(x) $ w otoczeniu $ x^* $ musi być różna od zera. No i $ x_0 $ musi być dostatecznie blisko $ x^* $. Wtedy metoda jest zbieżna, a błąd to: 
$$ e_{n+1}=\frac{f''(\xi_n)}{2f'(x_n)}e_n^2, \xi_n \in [x_n, x^*] $$
\subsubsection{Metoda Herona liczenia pierwiastka}
Niech $ a>0, x_0>0 $. 
$$ x_{n+1}=\frac12(x_n+\frac a{x_n}), x_n \rightarrow \sqrt a $$

Jest to metoda Newtona dla $ f(x)=x^2-a $, i jest zbieżna globalnie.
\paragraph{Twierdzenie.} Jeśli $ f \in C^2(\mathbb{R}) $ jest wypukła i rosnąca na $ \mathbb{R} $ oraz $ \exists_{x^*\in\mathbb{R}}f(x^*)=0 $ to metoda Newtona jest zbieżna dla dowolnego $ x_0\in\mathbb{R} $
\begin{proof}
	$$ f''>0, f'>0 $$
	$$x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}$$
	$$e_n=x_n-x^*$$
	$$e_{n+1}=\frac{f''(\xi_n)}{2f'(x_n)}e_n^2$$
	Wtedy $ e_{n+1}>0 $, czyli $ x>x^* $, stąd $ f(x_{n+1})>f(x^*)=0 $, zatem $ x_{n+1}<x_n $, czyli $ x_n $ jest ciągiem ograniczonym z dołu i ograniczonym, czyli zbieżnym.
\end{proof}
\subsection{Jak policzyć $\frac{1}{R}$ dla $ R>0 $ nie umiejąc dzielić?}
$ x-\frac1R $ - zbiega szybko, ale nic nie daje bo trzeba dzielić
$ R-\frac1x $, $ f(\frac1R)=0 $, $ f'(x)=-\frac1{x^2} $
$ x_{n+1} = x_n=\frac{\frac{1}{x_n}-R}{-\frac{1}{x^2_n}}=2x_n-Rx_n^2=x_n(2-Rx_n)$

\end{document}
