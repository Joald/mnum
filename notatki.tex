\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{polski}
\usepackage{textcomp}
\usepackage{color}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{amsthm}
\usepackage{amsmath}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
	language=Bash,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	escapeinside={(*}{*)},          % if you want to add LaTeX within your code
	tabsize=4
}

\title{Notatki z Metod numerycznych }
\author{Jacek Olczyk}
\date{October 2018}

\begin{document}
	\maketitle
	\part{Wykład}
	\section{Rozwiązywanie układów równań liniowych}
	\paragraph{Znane metody}
	\begin{itemize}
		\item $Ax=b, A\in\mathbb R^{n\times n}$
		\item Algorytm rokładem LU (elim. Gaussa) z wybraniem el. gł $O(\frac{2}{3}N^3)$.
		\item 1. złota myśl numeryka: Co zrobić jeśli zadanie jest za trudne? Zmienić zadanie
		\item Zamiast rozwiązywać układ równań, przybliżamy go
		\item Czy da się szybciej niż Gauss, który jest $O(n^3)$? To jak nisko da się zejść to problem otwarty, ale istnieją algorytmy lepsze niż sześcian.
	\end{itemize}

\section{Przybliżone rozwiązywanie układów równań}
\begin{itemize}
	\item Niech $ A=M-Z $, wtedy $ Ax=Mx-Zx=b $, zatem $ Mx=Zx+b $
	\item TODO Metoda iteracji prostej Banacha $Mx_{n+1} = Zx_{n}+b$
	\item Jeśli wybierzemy $ M $ tak, by układ z macierzą $ M $ można było tanio rozwiązać, wtedy iteracja też będzie tania
	\item Chcemy, żeby $ M $ było dobrym przybliżeniem $ A $, ale nie aż tak łatwo że
	\item Metoda Jacobiego $$a_{kk}x_{k}^{n+1}=b - \sum_{j\not=k} a_{kj}x_{j}^n$$
	\item inny pomysł, metoda Gaussa-Seidela: $a_{kk}x_k^{(n+1)} = b_k - \sum_{j<k}a_{kj}x_{j}^{(n+1)} - \sum_{j >k}a_{kj}x_{j}^{(n)}$
	\item Uwaga: fakt życiowy. Gdy $n$ jest bardzo duże, wówczas w $A$ jest zazwyczaj bardzo dużo zer, o ile układ pochodzi z $\text{REAL LIFE}^{TM}$.
	\item To oznacza, że ilość elementów różnych od $0$ jest rzędu $O(n)$. Mówimy wtedy że macierz jest rzadka.
	\item Wniosek: Jeśli $ A $ ma $ O(n) $ niezerowych elementów, to mnożenie $Ax$ kosztuje też $ O(n) $. Ponadto, rozwiązanie układu z macierzą dolnotrójkątną też jest $ O(n) $

\end{itemize}
\section{Normy macierzowe i wektorowe}
\paragraph{Normy wektorowe}
$$ ||x||_{p}:=(\sum_{i=1}^{N}|x_i|^p)^\frac1p $$
$$ ||x||_\infty:=\max_i |x_i| $$
\paragraph{Norma macierzowa}
$$ ||A||_p := \max_{x\not=0}\frac{||Ax||_p}{||x||_p}=\max_{||x||_p}||Ax||_p $$
\paragraph{Własności normy macierzowej}
\begin{enumerate}
	\item $$||Ax||\leq||A||\dotsm||x|| \forall_{x\in\mathbb{R}^n}$$
	\item $$ ||Ax|| $$ - nie dało się przeczytać tablicy
	\item tu też coś było :(
\end{enumerate}

\section{Warunek wystarczający zbieżności klasycznej metody iteracyjnej ($A=M-Z$)}
\begin{equation}
Mx_{k+1}=b+Zx_k\tag{$*$}
\end{equation}
Niech $x^*$ będzie dokładnym rozwiązaniem $Ax^*=b$
$$x_{k+1}=M^{-1}(b-Zx_k)$$
$$x_{k+1}-x^*=M^{-1}(b-Zx_k)-x^*$$
$$=M^{-1}(Ax^*-Zx_k)-x^*$$
$$=M^{-1}(Ax^*-(M-A)x_k)-x^*$$
$$=M^{-1}Ax^*+(I-M^{-1}A)x_k)-x^*$$
$$=-(I-M^{-1}A)x^*+(I-M^{-1}A)x_k)-x^*$$
$$=(I-M^{-1}A)(x^k-xu^*)$$
Czyli $B$ pomnożont błąd $k$-ty.\\
Czyli $ x_{n+1}-x^*=B(x_k-x^*)=B^2(x_k-x^*)\ldots=B^{k+1}(x_0-x^*) $

\paragraph{Wniosek:} Jeśli $ ||B||<1 $, to $(*)$ zbieżna do $x*$ dla dow. $x_0\in \mathbb{R}^N$

\paragraph{Twierdzenie:} Metoda $ (*) $ jest zbieżna do $ x^* $ z dowolnego $ x_0 $ wtw gdy $ \rho(B)<1 $
 gdzie $ \rho(B)=\max\{|\lambda|:\lambda \text{ jest wartością własną }B\} $ - promień spektralny macierzy $ B $
 Dowód pominięty

\paragraph{Twierdzenie:} Jeśli macierz $ A $ jest ściśle diagonalnie dominująca, tzn zachodzi
$ |a_n|>\sum_{j\not=i}|a_{ij}|\text{ dla }i=1..N $
 to metoda Jacobiego jest zbieżna (dla dowolnych $ x_n\in R^n $)
\begin{proof}
	Zbadajmy macierz iteracji.
	$$ ||B||_\infty = ||I-M^{-1}A||_\infty $$
	$ M^-1 $ dla macierzy diagonalnej to podnoszenie wszystkich elementów do $ -1 $.\\
	$ M^{-1}A=I $ + macierz z zerami na diagonali i ułamkami na reszcie, pierwszy wiersz to $ 0, a_{12}/a_{11}, a_{13}/a_{11}\ldots $\\
	Żeby uzyskać $ B $ odejmujemy $I$.\\
	$ ||B||_\infty = \max_i w_i  $\\
	$ w_i = \sum_j |b_{i,j}|=\sum_{j\not=i}|a_{ij}/a_{ji}|=\frac1{|a_{ii}|}\sum_{j\not=i}|a_{ij}|<1 $
	zatem norma $ B $ jest mniejsza od 1 więc normy są zbieżne.
\end{proof}

\section{Metody iteracyjne oparte na normalizacji w przestrzeni Kryłowa}
\paragraph{$k$-ta przestrzeń Kryłowa}
$$ K_k = {r_0, Ar_0, ...,A^{k-1}  r_0} $$
gdzie $ r_k:=b-Ax_k $ - reszta na $ k $-tej iteracji
\paragraph{Metoda iteracyjna}
\begin{itemize}
	\item $ x_k+1 \in K_k $ przesunięta o $ x_0 $
	\item $ x_k+1 $ normalizuje pewną miarę błędu na $ x_0+K_k $
	\item Na przykład: $$ ||x_k-X^*||_C\leq ||y-x^*||_C \forall_{y\in x_0+K_k} $$
	lub $$ ||r_k||\leq ||b-Ay||_C \forall_{y\in x_0+K_k} $$
	gdzie $ C =C^T>0 $
\end{itemize}

\subsection{Metoda gradientów sprzężonych (CG - Conjugate Gradient) dla macierzy $A=A^T>0$}
\subsubsection{Fakty o macierzach symetrycznych i dodatnio określonych}

Niech $A = A^T>0$ (symetryczna i dodatnio określona, a co za tym idzie $x^TAx>0\text{ dla }x\not=0$). Wtedy:
\begin{enumerate}
	\item Wartości własne są rzeczywiste a wektory własne są ortogonalne (czyli $ A=Q\Lambda Q^T $, gdzie $ Q $ jest ortogonalna, a $ \Lambda $ jest diagonalna)
	\item $ ||x||_A:=\sqrt{x^TAx} $ określa normę wektorową (norma energetyczna indukowana przez $ A $)
\end{enumerate}
Iterację metody gradientów sprzężonych definiujemy następująco:
$$ x_{k+1}\in x_0 + K_k $$
$$ ||x_{k+1}-x^*||_A\leq ||y-x^*||_A\forall_{y\in x_0+K_k} $$
Ale przecież potrzebujemy mieć rozwiązanie żeby to zrobić!
\paragraph{Fakt.} Można stąd wyprowadzić algorytm iteracyjny, który na podstawie kilku poprzednio wyznaczonych wektorów wyznaczy $ x_{k+1} $ kosztem jednego mnożenia przez macierz $ A $ i $ O(N) $
\paragraph{Twierdzenie.} Po $ k $ iteracjach metody CG błąd $ ||x_k-x^*||_A \leq 2(\frac{\sqrt{\alpha}-1}{\sqrt{\alpha}+1})^k ||x_0-x^*||_A $
gdzie $ \alpha = \lambda_{max}(A)/\lambda_{min}(A) $.

\section{Zagadnienia własne}

Dla $ A \in R^{NxN} $ znaleźć parę własną $ (\lambda, x)$, że $Ax = \lambda x$ oraz $x \neq 0$.
$\lambda$ pierwiastkiem wielomianu charakterystycznego: $det(A - \lambda I) = 0$
Gdy $A = A^T$ to wartości i wektory własne rzeczywiste, istnieje $Q$ ortogonalna $A = Q*L*Q^T$ (L to tylko lambdy na przekątnej)


3 podstawowe klasy zadań obliczeniowych dla zagadnień własnych:

1. ekstremalne wartości własne (największa, najmniejsza, etc) i odp. wektory (PageRank)

2. wartości własne bliskie zadanej wartości (wieżowce w Japonii)

3. pełne zadanie własne

Wyznaczanie wektora odpowiadającego dominującej wartości własnej (zakładamy że istnieje dokładnie jedna wartość własna że jej moduł ostro większy od innych modułów)

$$||A_x|| = ||\lambda*x|| = |\lambda| * ||x|| = |\lambda| $$ (bo $||x|| = 1$)

Metoda potęgowa
$x_0$ startowy o normie 1
$$x_{n+1} = A * x_n$$
$$x_{n+1} := x_{n+1}/||x_{n+1}|| $$
skąd nazwa:
$$x_{n+1} = A*x_n = A*Ax_{n-1} = A^2*x_{n-1} = ... = A^{n+1}*x_0 $$
nie robić tego w ten sposób, bo A jest duże (ale rzadkie) i będzie coraz mniej rzadkie!
Lepiej iteracyjnie, bo tanio mnożyć przez rzadką macierz

Twierdzenie o zbieżności tej metody:
Załóżmy, że A diagonalizowalna - istnieje Y nieosobliwe że
$YAY^{-1}$ tworzy macierz diagonalną

$A*y_i = \lambda*y_i$ gdzie $y_i$ to kolumna $Y$

$$ x_0 = \sum_{1}^{n} \alpha_i * y_i $$
$$x_n = A^n*x_0 = A^{n-1}*(A*x_0) = $$
$$ = A^{n-1}*\sum_{1}^{n} \alpha_i*y_i = $$
$$ = A^{n-1}*\sum_{1}^{n} \alpha_i*\lambda_i*y_i = $$
$$= \sum_{1}^{n} \alpha_i*\lambda_i^n*y_i = $$
$$= \lambda_1^n *\sum_{1}^{n} \alpha_i*()\lambda_i/\lambda_1)^n*y_i$$

Jeżeli $\lambda_1$ dominujące, to $\lambda_i/\lambda_1^n \rightarrow 0 $
($\lambda_1 \neq 0 $)

Odzyskanie wartości własnej na podstawie przybliżenia (znaleźć takie przybliżenie lambdy że norma przybliżenia $A*x-\lambda*x$  minimalna) - jest to zadanie najmniejszych kwadratów
iloraz Rayleigh TODO
Transformacje spektrum:
1. Jeżeli $\lambda$ ww $A$ to $\lambda-\mu$ ww $A-\mu*I$
2. Jeżeli $\lambda$ ww $A$ nieosobliwego to $1/(\lambda)$ ww $A^(-1)$

Odwrotna metoda potęgowa na zadania typu 2:

Wartosci wlasne $(A - \mu*I)^{-1}$ to $1/(\lambda-\mu) $
Kiedy największe? Kiedy $\mu$ blisko $\lambda_i$ to wtedy $1/(\lambda_i - \mu$ dominującą ww

RQI raileigh quotient iteration, bardzo szybko zbieżne ale niekoniecznie do najbliższego oryginałowi ww TODO

3. pełny problem - metoda QR TODO
%tak naprawdę metoda potęgowa nie na jednym wektorze a na wszystkich, zazwyczaj słabo działa, modyfikacja "raz dodajemy a raz odejmujemy"


\part{Ćwiczenia}
\section{Układy nadokreślone - kontynuacja}
\subsection{Zadanie 1.}
\paragraph{Macierz Hessenberga} - to macierz trójkątna górna, z tym że niezerowe elementy mogą być jeden element pod diagonalą.
$$\begin{bmatrix}
	x&\ldots&&&&x\\
	x&x&\ldots&&&x\\
	&x&x&\ldots&&x\\
	&&\ddots&&&x\\
	&&&x&x&x\\
	&&&&x&x\\
\end{bmatrix}$$
Jak najmniejszym kosztem znaleźć rozkład $ QR $ tej macierzy?\\
Metodą Householdera? Nie ma jak wykorzystać zer na dole.
\subsubsection{Obroty Givensa - przypomnienie}
\begin{enumerate}
	\item $ G_{ij} $ - macierz Givensa
	\item $ b=G_{ij}a$
	\item $ b_j=0 $
	\item $ \cos\phi = \frac{a_i}{\sqrt{a_i^2+a_j^2}} $
	\item $ \sin\phi = \frac{a_j}{\sqrt{a_i^2+a_j^2}} $
\end{enumerate}

\subsubsection{Zamiana macierzy Hessenberga w górnotrójkątną obrotami Givensa}
$$ (G_{ij}a)_j = -\frac{a_ia_j}{\sqrt{a_i^2 + a_j^2}}+\frac{a_ia_j}{\sqrt{a_i^2 + a_j^2}}=0$$
$$G_{n-1 n}\ldots G_{i i+1}\ldots G_{12}A=R$$
\subsubsection{Koszt}
Robimy $ n-1 $ iteracji.\\
Dla $ G_{i\ i+1} $ trzeba wykonać jeden pierwiastek, $ w_i =cw_i+sw_{i+1}$ oraz $w_{i+1} = -sw_i + cw_{i+1}$, łącznie $4(n-1) $ mnożeń.\\
Wszystko razem: $4\sum_{i=1}^{n-1}n-i=4\sum_{i=1}^{n-1}i=\frac{4n(n-1)}{2}\sim2n^2$.
\subsection{Zadanie 2.}
Dane są punkty $(-1, -1), (0,2), (1, 0), (2, 1)$.\\
Znajdź prostą $ y=ax+b $ najlepiej przybliżającą te punkty (w sensie LZNK).\\
Zadane punkty oznaczamy jako $ (x_i, y_i) $.\\
Zatem to, co chcemy zminimalizować to $y(x_i)-y_i$.\\
Policzmy normę: $ \min_{a, b} \sum_{i=1}^{4}(y(x_i)-y_i)^2$\\
Niewiadome to $ a $ oraz $ b $, więc niech:
$$
\begin{matrix}
	z=\begin{bmatrix}a\\b\end{bmatrix}&
	d=\begin{bmatrix}y_i\end{bmatrix}_{i=1, 2, 3, 4}&
	A=\begin{bmatrix}
		1&x_1\\
		1&x_2\\
		1&x_3\\
		1&x_4\\
	\end{bmatrix}
\end{matrix}
$$
Teraz wystarczy użyć LZNK aby obliczyć $\min{||Az-d||_2}$:
\paragraph{TODO:} policzyć to
\paragraph{Uwaga:} w ten sposób odległość między punktami a prostą liczymy w pionie, a nie najbliższą (to dobrze, tak działą LZNK).
\paragraph{Uwaga 2:} LZNK nie działa dla równania $ y=a+e^{bx} $, ale dla $ y=a+be^x $ już tak!
\section{Normy}
\paragraph{Przypomnienie definicji: } Norma $ ||\cdot||: V\rightarrow\mathbb{R}^+$ spełnia następujące warunki:
\begin{enumerate}
	\item $||u+v||\leq||u||+||v||$
	\item $||\alpha v||=||av||$
	\item $||v||=0\implies v=0$ - wektor zerowy
\end{enumerate}
\paragraph{$p$-te normy wektorowe:}
$$||x||_p = \sqrt[p]{\sum_{i=1}^{n}|x_i|^p}$$
$$||x||_\infty = \max_i |x_i|$$
\paragraph{Normy macierzowe}
Niech $A\in \mathbb{R}^{n\times n}$.\\
Macierzowe normy indukowane są postaci
$$||A||=\sup_{x\not=0}\frac{||Ax||}{||x||}=\sup_{||x||=1}||Ax||$$
\subparagraph{$p$-te normy macierzowe}
$$||A||_p=\sup_{||x||_p=1}||Ax||_p, p=1, 2, \ldots, \infty$$
wszystkie poza $1, 2, \infty$ zwykle się pomija
\subsection{Własności norm indukowanych macierzy}
\begin{enumerate}
	\item $||Ax||\leq||A||||x||$ - z definicji mamy $ ||A||\geq\frac{||Ax||}{||x||} $
	\item $ ||AB||\leq||A||||B||, A,B\in \mathbb{R}^{n\times n} $ - bo
	$$ ||ABx||\leq||A||||Bx||\leq||A||||B||||x|| $$ oraz
	$$ ||AB||=\sup_{x\not=0}\frac{||ABx||}{||x||}\leq||A||||B|| $$
\end{enumerate}
\paragraph{Fakt.} W przestrzeniach skończonego wymiaru wszystkie normy spełniają równanie:
$ \exists_{c_1, c_2>0}\forall_{x}c_1||x||_1\leq||x||2\leq c_2||x||_1 $, gdzie normy są dowolne (niekoniecznie pierwsza i druga)

\subsection{Zależności między normami}
Niech $ x\in\mathbb{R}^n $, a normy będą $p$-te.

$$ ||x||^2_1=(\sum_i|x_i|)^2 \ge ||x||_2^2$$
$$ ||x||_1\le \alpha||x||_2 $$
Jakie $\alpha$ wybrać?
$$ ||x||_1\ge ||x||_\infty $$
$$ n||x||_\infty\ge ||x||_1 $$
$$ ||x||_\infty\le ||x||_2 $$
$$ \sqrt n||x||_\infty\ge ||x||_2 $$
$$ ||x||_1\leq n||x||_\infty\leq n||x||_2 $$
Zatem $\alpha=n$.
\paragraph{Nierówności}
$ \frac1n ||A||_2\le\frac1{\sqrt n}||A||_\infty\le||A||_2\le\sqrt{n} ||A||_1\le n||A||_2$
\subsection{Wzory na normy macierzowe}
$$ ||A||_1=\max_j\sum_{i=1}^{n} |a_{ij}|$$
$$ ||A||_\infty=\max_i\sum_{j=1}^{n} |a_{ij}|$$
Zatem $ ||A^T||_1=||A||_\infty $.\\
\paragraph{Norma druga (spektralna)}
$$||A||_2=\max_{\lambda\in\delta(A^TA)}\sqrt{\lambda}$$
Gdzie $ \delta(M) $ jest zbiorem wartości własnych macierzy $ M $.\\
Jeśli Q ortogonalna: $||Q||_2=1$, co za tym idzie $ ||I||_2=1 $
$ \lambda $- wartość własna oraz $ v $ - wektor własny spełnieją $ Av=\lambda v $
\paragraph{Norma Frobeniusa (Euklidesowa)}
$$ ||A||_F = \sqrt{\sum_{i,j}|a_{ij}|} $$
Nie jest normą indukowaną, bo dla wszystkich norm indukowanych zachodzi $ ||I||=\sup_{x\not=0}\frac{||Ix||}{||x||}=1 $, a
$ ||I||_F=\sqrt{n} $ - zatem nie pochodzi od drugiej normy wektorowej!
\end{document}
